{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4633cfb-42e8-41b6-bb91-d9b3fff30a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# AI PODCAST INTELLIGENCE SYSTEM\n",
    "# JupyterLab Production Version\n",
    "# ========================================\n",
    "\n",
    "# CELL 1: Import Dependencies and Setup\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import sqlite3\n",
    "import feedparser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ea98b0-098d-451a-ac80-bf627eddf381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß LOADING CONFIGURATION...\n",
      "‚úÖ Configuration loaded successfully\n",
      "‚úÖ Anthropic API key found\n",
      "üìª Active podcasts configured: 8\n",
      "   ‚Ä¢ Practical AI (high priority)\n",
      "   ‚Ä¢ Last Week in AI (high priority)\n",
      "   ‚Ä¢ The AI Podcast (medium priority)\n",
      "   ‚Ä¢ Lex Fridman Podcast (high priority)\n",
      "   ‚Ä¢ Eye on AI (high priority)\n",
      "   ‚Ä¢ AI Today Podcast (medium priority)\n",
      "   ‚Ä¢ The AI Show (medium priority)\n",
      "   ‚Ä¢ MIT AI Podcast (medium priority)\n",
      "üí∞ Estimated weekly cost: ~$0.27\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load Configuration and Verify Setup\n",
    "print(\"\\nüîß LOADING CONFIGURATION...\")\n",
    "\n",
    "try:\n",
    "    # Load YAML configuration\n",
    "    with open('podcast_config.yaml', 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Get API key from environment\n",
    "    anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "    \n",
    "    if anthropic_api_key:\n",
    "        print(\"‚úÖ Configuration loaded successfully\")\n",
    "        print(\"‚úÖ Anthropic API key found\")\n",
    "    else:\n",
    "        print(\"‚ùå Anthropic API key not found in .env file\")\n",
    "        print(\"Please add your API key to the .env file\")\n",
    "    \n",
    "    # Show active podcasts\n",
    "    active_podcasts = {name: cfg for name, cfg in config['podcasts'].items() if cfg.get('active', True)}\n",
    "    print(f\"üìª Active podcasts configured: {len(active_podcasts)}\")\n",
    "    \n",
    "    for name, cfg in active_podcasts.items():\n",
    "        print(f\"   ‚Ä¢ {name} ({cfg.get('priority', 'medium')} priority)\")\n",
    "    \n",
    "    # Estimate weekly cost\n",
    "    total_episodes = sum(cfg.get('estimated_episodes_per_week', 1) for cfg in active_podcasts.values())\n",
    "    estimated_cost = total_episodes * 0.03  # Rough estimate\n",
    "    print(f\"üí∞ Estimated weekly cost: ~${estimated_cost:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    config = None\n",
    "    anthropic_api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f84b08c-4483-4e02-9464-c2bc0d6d0bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced save_intelligence method applied successfully!\n",
      "\n",
      "üîß INITIALIZING PRODUCTION SYSTEM...\n",
      "2025-09-23 20:54:57,262 - PodcastIntelligence - INFO - Database setup completed\n",
      "2025-09-23 20:54:57,264 - PodcastIntelligence - INFO - Production system initialized with 9 active podcasts\n",
      "‚úÖ Production system ready!\n",
      "\n",
      "üìä SYSTEM OVERVIEW:\n",
      "üìª Total episodes: 8\n",
      "üß† Processed: 9 (112.5%)\n",
      "üí∞ Total cost: $0.2084\n",
      "‚≠ê Avg importance: 6.9/10\n",
      "üìà Recent: 6 episodes this week\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_intelligence_no_duplicates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1056\u001b[39m\n\u001b[32m   1052\u001b[39m     system = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# Apply the fix\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m system.save_intelligence = \u001b[43msave_intelligence_no_duplicates\u001b[49m.\u001b[34m__get__\u001b[39m(system, ProductionPodcastIntelligence)\n\u001b[32m   1057\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Enhanced save_intelligence method applied!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'save_intelligence_no_duplicates' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 3: Production AI Podcast Intelligence System Class - Complete Enhanced Version\n",
    "class ProductionPodcastIntelligence:\n",
    "    \"\"\"Production-ready AI Podcast Intelligence System for JupyterLab\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dict: Dict):\n",
    "        self.config = config_dict\n",
    "        self.db_path = config_dict['system']['database_path']\n",
    "        self.reports_dir = Path(config_dict['system']['reports_directory'])\n",
    "        self.logs_dir = Path(config_dict['system']['logs_directory'])\n",
    "        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.reports_dir.mkdir(exist_ok=True)\n",
    "        self.logs_dir.mkdir(exist_ok=True)\n",
    "        Path(self.db_path).parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Initialize database\n",
    "        self.setup_database()\n",
    "        \n",
    "        # Update database schema for enhanced features\n",
    "        self.update_database_schema()\n",
    "        \n",
    "        self.logger.info(f\"Production system initialized with {len(self.get_active_podcasts())} active podcasts\")\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup production logging\"\"\"\n",
    "        log_file = self.logs_dir / f\"podcast_intelligence_{datetime.now().strftime('%Y%m%d')}.log\"\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, self.config['system']['log_level']),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.logger = logging.getLogger('PodcastIntelligence')\n",
    "    \n",
    "    def setup_database(self):\n",
    "        \"\"\"Create database tables with enhanced schema\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Episodes table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS episodes (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                podcast_name TEXT NOT NULL,\n",
    "                title TEXT NOT NULL,\n",
    "                pub_date TEXT,\n",
    "                description TEXT,\n",
    "                audio_url TEXT,\n",
    "                episode_url TEXT,\n",
    "                duration_minutes INTEGER,\n",
    "                guid TEXT,\n",
    "                processed BOOLEAN DEFAULT 0,\n",
    "                processing_attempts INTEGER DEFAULT 0,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                UNIQUE(podcast_name, title)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Intelligence table with enhanced fields\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS intelligence (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                episode_id INTEGER,\n",
    "                headline_takeaway TEXT,\n",
    "                executive_summary TEXT,\n",
    "                strategic_implications TEXT,\n",
    "                technical_developments TEXT,\n",
    "                market_dynamics TEXT,\n",
    "                key_people TEXT,\n",
    "                companies_mentioned TEXT,\n",
    "                predictions TEXT,\n",
    "                actionable_insights TEXT,\n",
    "                risk_factors TEXT,\n",
    "                quantified_impact TEXT,\n",
    "                bottom_line TEXT,\n",
    "                guest_expertise TEXT,\n",
    "                importance_score INTEGER,\n",
    "                confidence_score REAL,\n",
    "                processing_cost REAL,\n",
    "                processing_time_seconds REAL,\n",
    "                model_used TEXT,\n",
    "                episode_url TEXT,\n",
    "                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (episode_id) REFERENCES episodes (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Processing logs table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS processing_logs (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                operation_type TEXT,\n",
    "                podcast_name TEXT,\n",
    "                episode_title TEXT,\n",
    "                status TEXT,\n",
    "                error_message TEXT,\n",
    "                cost REAL,\n",
    "                processing_time REAL,\n",
    "                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Reports table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS reports (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                report_type TEXT,\n",
    "                report_period TEXT,\n",
    "                episodes_count INTEGER,\n",
    "                total_cost REAL,\n",
    "                file_path TEXT,\n",
    "                generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        self.logger.info(\"Database setup completed\")\n",
    "    \n",
    "    def update_database_schema(self):\n",
    "        \"\"\"Update database schema to include all enhanced fields\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Check existing columns in episodes table\n",
    "            cursor.execute(\"PRAGMA table_info(episodes)\")\n",
    "            existing_columns = {column[1] for column in cursor.fetchall()}\n",
    "            \n",
    "            # Add missing columns to episodes table\n",
    "            new_episode_columns = {\n",
    "                'episode_url': 'TEXT',\n",
    "                'guid': 'TEXT',\n",
    "                'duration_minutes': 'INTEGER'\n",
    "            }\n",
    "            \n",
    "            for column, data_type in new_episode_columns.items():\n",
    "                if column not in existing_columns:\n",
    "                    cursor.execute(f'ALTER TABLE episodes ADD COLUMN {column} {data_type}')\n",
    "                    self.logger.info(f\"Added {column} column to episodes table\")\n",
    "            \n",
    "            # Check existing columns in intelligence table\n",
    "            cursor.execute(\"PRAGMA table_info(intelligence)\")\n",
    "            existing_intel_columns = {column[1] for column in cursor.fetchall()}\n",
    "            \n",
    "            # Add missing columns to intelligence table\n",
    "            new_intel_columns = {\n",
    "                'headline_takeaway': 'TEXT',\n",
    "                'strategic_implications': 'TEXT',\n",
    "                'market_dynamics': 'TEXT',\n",
    "                'actionable_insights': 'TEXT',\n",
    "                'risk_factors': 'TEXT',\n",
    "                'quantified_impact': 'TEXT',\n",
    "                'bottom_line': 'TEXT',\n",
    "                'guest_expertise': 'TEXT',\n",
    "                'episode_url': 'TEXT'\n",
    "            }\n",
    "            \n",
    "            for column, data_type in new_intel_columns.items():\n",
    "                if column not in existing_intel_columns:\n",
    "                    cursor.execute(f'ALTER TABLE intelligence ADD COLUMN {column} {data_type}')\n",
    "                    self.logger.info(f\"Added {column} column to intelligence table\")\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error updating database schema: {e}\")\n",
    "            conn.close()\n",
    "    \n",
    "    def get_active_podcasts(self) -> Dict:\n",
    "        \"\"\"Get active podcasts from configuration\"\"\"\n",
    "        return {name: cfg for name, cfg in self.config['podcasts'].items() if cfg.get('active', True)}\n",
    "    \n",
    "    def fetch_recent_episodes(self, podcast_name: str, podcast_config: Dict) -> List[Dict]:\n",
    "        \"\"\"Enhanced episode fetching with better URL extraction\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Fetching episodes from {podcast_name}\")\n",
    "            feed = feedparser.parse(podcast_config['rss_url'])\n",
    "            \n",
    "            if not feed.entries:\n",
    "                self.logger.warning(f\"No episodes found for {podcast_name}\")\n",
    "                return []\n",
    "            \n",
    "            days_back = self.config['system']['days_lookback']\n",
    "            max_episodes = self.config['system']['max_episodes_per_podcast']\n",
    "            since_date = datetime.now() - timedelta(days=days_back)\n",
    "            \n",
    "            new_episodes = []\n",
    "            for entry in feed.entries[:max_episodes]:\n",
    "                try:\n",
    "                    # Parse publication date\n",
    "                    pub_date = datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else datetime.now()\n",
    "                    \n",
    "                    if pub_date >= since_date:\n",
    "                        # Enhanced URL extraction\n",
    "                        episode_url = self.extract_episode_url(entry)\n",
    "                        audio_url = self.extract_audio_url(entry)\n",
    "                        \n",
    "                        episode_data = {\n",
    "                            'podcast_name': podcast_name,\n",
    "                            'title': entry.title,\n",
    "                            'pub_date': pub_date.isoformat(),\n",
    "                            'description': getattr(entry, 'description', '') or getattr(entry, 'summary', ''),\n",
    "                            'audio_url': audio_url,\n",
    "                            'episode_url': episode_url,\n",
    "                            'duration_minutes': self.extract_duration(entry),\n",
    "                            'guid': getattr(entry, 'guid', getattr(entry, 'id', ''))\n",
    "                        }\n",
    "                        new_episodes.append(episode_data)\n",
    "                        \n",
    "                        self.logger.debug(f\"Episode URL found: {episode_url}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing episode from {podcast_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.logger.info(f\"Found {len(new_episodes)} new episodes from {podcast_name}\")\n",
    "            return new_episodes\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching {podcast_name}: {e}\")\n",
    "            self.log_operation('fetch_episodes', podcast_name, '', 'error', str(e))\n",
    "            return []\n",
    "    \n",
    "    def extract_episode_url(self, entry) -> Optional[str]:\n",
    "        \"\"\"Extract the main episode page URL (not audio file)\"\"\"\n",
    "        # Priority order for finding episode URLs:\n",
    "        \n",
    "        # 1. Try the main link (usually the episode page)\n",
    "        if hasattr(entry, 'link') and entry.link:\n",
    "            return entry.link\n",
    "        \n",
    "        # 2. Try links array for non-audio links\n",
    "        if hasattr(entry, 'links'):\n",
    "            for link in entry.links:\n",
    "                link_type = link.get('type', '').lower()\n",
    "                # Skip audio files, look for HTML pages\n",
    "                if link_type in ['text/html', 'application/xhtml+xml', ''] or 'audio' not in link_type:\n",
    "                    if link.get('href'):\n",
    "                        return link.get('href')\n",
    "        \n",
    "        # 3. Try guid if it's a URL\n",
    "        if hasattr(entry, 'guid'):\n",
    "            guid = entry.guid\n",
    "            if isinstance(guid, str) and (guid.startswith('http') or guid.startswith('www')):\n",
    "                return guid\n",
    "        \n",
    "        # 4. Try id field\n",
    "        if hasattr(entry, 'id'):\n",
    "            entry_id = entry.id\n",
    "            if isinstance(entry_id, str) and (entry_id.startswith('http') or entry_id.startswith('www')):\n",
    "                return entry_id\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_audio_url(self, entry) -> Optional[str]:\n",
    "        \"\"\"Extract audio URL from RSS entry\"\"\"\n",
    "        # Look for audio enclosures\n",
    "        if hasattr(entry, 'enclosures'):\n",
    "            for enclosure in entry.enclosures:\n",
    "                if 'audio' in enclosure.get('type', ''):\n",
    "                    return enclosure.get('href')\n",
    "        \n",
    "        # Fallback to links\n",
    "        if hasattr(entry, 'links'):\n",
    "            for link in entry.links:\n",
    "                if 'audio' in link.get('type', ''):\n",
    "                    return link.get('href')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_duration(self, entry) -> Optional[int]:\n",
    "        \"\"\"Extract episode duration in minutes\"\"\"\n",
    "        # Try to extract duration from various fields\n",
    "        duration_str = getattr(entry, 'itunes_duration', '') or getattr(entry, 'duration', '')\n",
    "        \n",
    "        if duration_str:\n",
    "            try:\n",
    "                # Parse duration (format: HH:MM:SS or MM:SS)\n",
    "                parts = duration_str.split(':')\n",
    "                if len(parts) == 3:  # HH:MM:SS\n",
    "                    return int(parts[0]) * 60 + int(parts[1])\n",
    "                elif len(parts) == 2:  # MM:SS\n",
    "                    return int(parts[0])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_intelligence(self, transcript: str, podcast_name: str, episode_title: str, podcast_config: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Enhanced intelligence extraction with executive communication principles\"\"\"\n",
    "        if not self.anthropic_api_key or not transcript:\n",
    "            self.logger.warning(f\"Missing API key or transcript for {episode_title}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            focus = podcast_config.get('focus', 'general')\n",
    "            priority = podcast_config.get('priority', 'medium')\n",
    "            \n",
    "            # Get focus-specific extraction guidance\n",
    "            focus_config = self.config.get('focus_areas', {}).get(focus, {})\n",
    "            extraction_emphasis = focus_config.get('extraction_emphasis', \"Focus on key insights and actionable information\")\n",
    "            \n",
    "            # Enhanced prompt with executive communication principles\n",
    "            prompt = f\"\"\"\n",
    "            As a senior AI industry analyst, analyze this {podcast_name} episode and create an executive-level intelligence brief.\n",
    "            \n",
    "            PODCAST CONTEXT:\n",
    "            - Name: {podcast_name}\n",
    "            - Focus Area: {focus}\n",
    "            - Extraction Emphasis: {extraction_emphasis}\n",
    "            - Episode: {episode_title}\n",
    "            \n",
    "            TRANSCRIPT CONTENT:\n",
    "            {transcript[:4500]}\n",
    "            \n",
    "            Create a comprehensive analysis following executive communication best practices:\n",
    "            \n",
    "            1. START WITH HEADLINE: Lead with the most important takeaway\n",
    "            2. DESIGN FOR SKIMMING: Use clear structure and bold key points  \n",
    "            3. ANSWER OBJECTIONS: Address potential concerns upfront\n",
    "            4. SHOW IMPACT: Quantify business implications where possible\n",
    "            \n",
    "            Provide your analysis in this exact JSON format (NO markdown, NO code blocks, just clean JSON):\n",
    "            {{\n",
    "                \"headline_takeaway\": \"One powerful sentence capturing the most important insight from this episode\",\n",
    "                \"executive_summary\": \"A comprehensive 4-6 sentence summary that starts with the headline version, then provides crucial context. Focus on business impact, strategic implications, and actionable insights. Write for executives who scan quickly - each sentence should add clear value.\",\n",
    "                \"strategic_implications\": [\"3-4 high-level business or industry implications that executives should understand\"],\n",
    "                \"technical_developments\": [\"Specific technical advances, tools, frameworks, or breakthroughs mentioned with business context\"],\n",
    "                \"market_dynamics\": [\"Business trends, competitive insights, market shifts, or economic implications discussed\"],\n",
    "                \"key_people\": [\"Notable people mentioned in format: 'Name (Role at Company) - Key contribution or quote'\"],\n",
    "                \"companies_mentioned\": [\"Companies discussed with specific context: 'Company Name - What was said about them'\"],\n",
    "                \"predictions\": [\"Future predictions with timelines and confidence indicators where mentioned\"],\n",
    "                \"actionable_insights\": [\"4-6 specific, implementable takeaways that listeners can act on\"],\n",
    "                \"risk_factors\": [\"Potential challenges, risks, or concerns mentioned that could impact strategy\"],\n",
    "                \"quantified_impact\": [\"Any specific numbers, percentages, timelines, or measurable outcomes mentioned\"],\n",
    "                \"bottom_line\": \"One sentence that captures the core message: what should executives remember from this episode?\",\n",
    "                \"importance_score\": 8,\n",
    "                \"confidence_score\": 0.9,\n",
    "                \"guest_expertise\": \"Brief description of the main speaker's background and why their perspective matters\"\n",
    "            }}\n",
    "            \n",
    "            CRITICAL REQUIREMENTS:\n",
    "            - Executive summary must be 4-6 substantive sentences minimum\n",
    "            - Focus on business impact and strategic value\n",
    "            - Use clear, decisive language\n",
    "            - Quantify impact where possible\n",
    "            - Address potential objections or concerns\n",
    "            - Return clean JSON only - no formatting, no code blocks, no extra text\n",
    "            \"\"\"\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Call Claude API with enhanced settings\n",
    "            response = requests.post(\n",
    "                \"https://api.anthropic.com/v1/messages\",\n",
    "                headers={\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"x-api-key\": self.anthropic_api_key,\n",
    "                    \"anthropic-version\": \"2023-06-01\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": \"claude-sonnet-4-20250514\",\n",
    "                    \"max_tokens\": 3500,\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                claude_response = result['content'][0]['text'].strip()\n",
    "                \n",
    "                # Clean up any markdown formatting that might appear\n",
    "                if claude_response.startswith('```json'):\n",
    "                    claude_response = claude_response.replace('```json', '').replace('```', '').strip()\n",
    "                if claude_response.startswith('```'):\n",
    "                    claude_response = claude_response.replace('```', '').strip()\n",
    "                \n",
    "                # Calculate processing cost\n",
    "                input_tokens = len(prompt) / 4\n",
    "                output_tokens = len(claude_response) / 4\n",
    "                cost = (input_tokens * 3 + output_tokens * 15) / 1_000_000\n",
    "                \n",
    "                try:\n",
    "                    # Parse JSON response\n",
    "                    intelligence_data = json.loads(claude_response)\n",
    "                    \n",
    "                    # Add metadata\n",
    "                    intelligence_data.update({\n",
    "                        'processing_cost': cost,\n",
    "                        'processing_time_seconds': processing_time,\n",
    "                        'model_used': 'claude-sonnet-4-20250514',\n",
    "                        'extraction_timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "                    \n",
    "                    self.logger.info(f\"Successfully extracted enhanced intelligence from {episode_title} (${cost:.4f}, {processing_time:.1f}s)\")\n",
    "                    self.log_operation('extract_intelligence', podcast_name, episode_title, 'success', cost=cost, processing_time=processing_time)\n",
    "                    \n",
    "                    return intelligence_data\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    self.logger.warning(f\"Failed to parse JSON response for {episode_title}: {e}\")\n",
    "                    self.logger.debug(f\"Raw response: {claude_response[:200]}...\")\n",
    "                    \n",
    "                    # Fallback response with executive summary\n",
    "                    fallback_data = {\n",
    "                        'headline_takeaway': f\"Analysis of {episode_title} from {podcast_name}\",\n",
    "                        'executive_summary': claude_response[:800] + \"...\" if len(claude_response) > 800 else claude_response,\n",
    "                        'processing_cost': cost,\n",
    "                        'processing_time_seconds': processing_time,\n",
    "                        'model_used': 'claude-sonnet-4-20250514',\n",
    "                        'importance_score': 5,\n",
    "                        'confidence_score': 0.5,\n",
    "                        'bottom_line': 'Raw analysis available in executive summary due to parsing issue',\n",
    "                        'parsing_error': True\n",
    "                    }\n",
    "                    \n",
    "                    return fallback_data\n",
    "                    \n",
    "            else:\n",
    "                error_msg = f\"Claude API error: {response.status_code} - {response.text}\"\n",
    "                self.logger.error(error_msg)\n",
    "                self.log_operation('extract_intelligence', podcast_name, episode_title, 'error', error_message=error_msg)\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Intelligence extraction error: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            self.log_operation('extract_intelligence', podcast_name, episode_title, 'error', error_message=error_msg)\n",
    "            return None\n",
    "    \n",
    "    def save_episode(self, episode_data: Dict) -> Optional[int]:\n",
    "        \"\"\"Save episode to database with enhanced metadata\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT OR IGNORE INTO episodes \n",
    "                (podcast_name, title, pub_date, description, audio_url, episode_url, duration_minutes, guid)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                episode_data['podcast_name'],\n",
    "                episode_data['title'],\n",
    "                episode_data['pub_date'],\n",
    "                episode_data['description'],\n",
    "                episode_data.get('audio_url'),\n",
    "                episode_data.get('episode_url'),\n",
    "                episode_data.get('duration_minutes'),\n",
    "                episode_data.get('guid')\n",
    "            ))\n",
    "            \n",
    "            # Get episode ID\n",
    "            cursor.execute('''\n",
    "                SELECT id FROM episodes WHERE podcast_name = ? AND title = ?\n",
    "            ''', (episode_data['podcast_name'], episode_data['title']))\n",
    "            \n",
    "            result = cursor.fetchone()\n",
    "            episode_id = result[0] if result else None\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            \n",
    "            if episode_id:\n",
    "                self.logger.debug(f\"Saved episode: {episode_data['title']}\")\n",
    "            \n",
    "            return episode_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving episode: {e}\")\n",
    "            conn.close()\n",
    "            return None\n",
    "    \n",
    "    # Recreate the enhanced save_intelligence method\n",
    "    def save_intelligence(self, episode_id: int, intelligence_data: Dict):\n",
    "        \"\"\"Save intelligence data, replacing any existing record for this episode\"\"\"\n",
    "        import json\n",
    "        import sqlite3\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Check if intelligence already exists for this episode\n",
    "            cursor.execute('SELECT id FROM intelligence WHERE episode_id = ?', (episode_id,))\n",
    "            existing = cursor.fetchone()\n",
    "            \n",
    "            if existing:\n",
    "                intelligence_id = existing[0]\n",
    "                self.logger.info(f\"‚ö†Ô∏è Intelligence already exists for episode {episode_id}, updating...\")\n",
    "                \n",
    "                # Update existing record\n",
    "                cursor.execute('''\n",
    "                    UPDATE intelligence SET\n",
    "                        headline_takeaway = ?, executive_summary = ?, strategic_implications = ?,\n",
    "                        technical_developments = ?, market_dynamics = ?, key_people = ?, \n",
    "                        companies_mentioned = ?, predictions = ?, actionable_insights = ?,\n",
    "                        risk_factors = ?, quantified_impact = ?, bottom_line = ?, \n",
    "                        guest_expertise = ?, importance_score = ?, confidence_score = ?, \n",
    "                        processing_cost = ?, processing_time_seconds = ?, model_used = ?,\n",
    "                        episode_url = ?, processed_at = CURRENT_TIMESTAMP\n",
    "                    WHERE episode_id = ?\n",
    "                ''', (\n",
    "                    intelligence_data.get('headline_takeaway', ''),\n",
    "                    intelligence_data.get('executive_summary', ''),\n",
    "                    json.dumps(intelligence_data.get('strategic_implications', [])),\n",
    "                    json.dumps(intelligence_data.get('technical_developments', [])),\n",
    "                    json.dumps(intelligence_data.get('market_dynamics', [])),\n",
    "                    json.dumps(intelligence_data.get('key_people', [])),\n",
    "                    json.dumps(intelligence_data.get('companies_mentioned', [])),\n",
    "                    json.dumps(intelligence_data.get('predictions', [])),\n",
    "                    json.dumps(intelligence_data.get('actionable_insights', [])),\n",
    "                    json.dumps(intelligence_data.get('risk_factors', [])),\n",
    "                    json.dumps(intelligence_data.get('quantified_impact', [])),\n",
    "                    intelligence_data.get('bottom_line', ''),\n",
    "                    intelligence_data.get('guest_expertise', ''),\n",
    "                    int(intelligence_data.get('importance_score', 5)),\n",
    "                    float(intelligence_data.get('confidence_score', 0.5)),\n",
    "                    intelligence_data.get('processing_cost', 0.0),\n",
    "                    intelligence_data.get('processing_time_seconds', 0.0),\n",
    "                    intelligence_data.get('model_used', 'unknown'),\n",
    "                    intelligence_data.get('episode_url', ''),\n",
    "                    episode_id\n",
    "                ))\n",
    "            else:\n",
    "                # Insert new record\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO intelligence (\n",
    "                        episode_id, headline_takeaway, executive_summary, strategic_implications,\n",
    "                        technical_developments, market_dynamics, key_people, companies_mentioned,\n",
    "                        predictions, actionable_insights, risk_factors, quantified_impact,\n",
    "                        bottom_line, guest_expertise, importance_score, confidence_score, \n",
    "                        processing_cost, processing_time_seconds, model_used, episode_url\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    episode_id,\n",
    "                    intelligence_data.get('headline_takeaway', ''),\n",
    "                    intelligence_data.get('executive_summary', ''),\n",
    "                    json.dumps(intelligence_data.get('strategic_implications', [])),\n",
    "                    json.dumps(intelligence_data.get('technical_developments', [])),\n",
    "                    json.dumps(intelligence_data.get('market_dynamics', [])),\n",
    "                    json.dumps(intelligence_data.get('key_people', [])),\n",
    "                    json.dumps(intelligence_data.get('companies_mentioned', [])),\n",
    "                    json.dumps(intelligence_data.get('predictions', [])),\n",
    "                    json.dumps(intelligence_data.get('actionable_insights', [])),\n",
    "                    json.dumps(intelligence_data.get('risk_factors', [])),\n",
    "                    json.dumps(intelligence_data.get('quantified_impact', [])),\n",
    "                    intelligence_data.get('bottom_line', ''),\n",
    "                    intelligence_data.get('guest_expertise', ''),\n",
    "                    int(intelligence_data.get('importance_score', 5)),\n",
    "                    float(intelligence_data.get('confidence_score', 0.5)),\n",
    "                    intelligence_data.get('processing_cost', 0.0),\n",
    "                    intelligence_data.get('processing_time_seconds', 0.0),\n",
    "                    intelligence_data.get('model_used', 'unknown'),\n",
    "                    intelligence_data.get('episode_url', '')\n",
    "                ))\n",
    "            \n",
    "            # Mark episode as processed\n",
    "            cursor.execute('UPDATE episodes SET processed = 1, updated_at = CURRENT_TIMESTAMP WHERE id = ?', (episode_id,))\n",
    "            \n",
    "            conn.commit()\n",
    "            self.logger.debug(f\"Intelligence saved for episode ID: {episode_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving intelligence: {e}\")\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    \n",
    "    def log_operation(self, operation_type: str, podcast_name: str, episode_title: str, \n",
    "                     status: str, error_message: str = None, cost: float = 0.0, processing_time: float = 0.0):\n",
    "        \"\"\"Log system operations for monitoring\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO processing_logs \n",
    "                (operation_type, podcast_name, episode_title, status, error_message, cost, processing_time)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (operation_type, podcast_name, episode_title, status, error_message, cost, processing_time))\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error logging operation: {e}\")\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    \n",
    "    def check_cost_limits(self, estimated_cost: float) -> bool:\n",
    "        \"\"\"Check if processing would exceed cost limits\"\"\"\n",
    "        today = datetime.now().date()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get today's costs\n",
    "        cursor.execute('''\n",
    "            SELECT COALESCE(SUM(processing_cost), 0) FROM intelligence \n",
    "            WHERE DATE(processed_at) = ?\n",
    "        ''', (today,))\n",
    "        daily_cost = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get this week's costs\n",
    "        week_start = today - timedelta(days=today.weekday())\n",
    "        cursor.execute('''\n",
    "            SELECT COALESCE(SUM(processing_cost), 0) FROM intelligence \n",
    "            WHERE DATE(processed_at) >= ?\n",
    "        ''', (week_start,))\n",
    "        weekly_cost = cursor.fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Check limits\n",
    "        daily_limit = self.config['cost_limits']['daily_max_usd']\n",
    "        weekly_limit = self.config['cost_limits']['weekly_max_usd']\n",
    "        \n",
    "        if daily_cost + estimated_cost > daily_limit:\n",
    "            self.logger.warning(f\"Daily cost limit would be exceeded: ${daily_cost + estimated_cost:.4f} > ${daily_limit}\")\n",
    "            return False\n",
    "        \n",
    "        if weekly_cost + estimated_cost > weekly_limit:\n",
    "            self.logger.warning(f\"Weekly cost limit would be exceeded: ${weekly_cost + estimated_cost:.4f} > ${weekly_limit}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process_all_podcasts(self) -> Dict:\n",
    "        \"\"\"Enhanced podcast processing with comprehensive monitoring\"\"\"\n",
    "        self.logger.info(\"=== Starting enhanced podcast processing batch ===\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        active_podcasts = self.get_active_podcasts()\n",
    "        estimated_cost = len(active_podcasts) * 0.05  # Rough estimate\n",
    "        \n",
    "        self.logger.info(f\"Processing {len(active_podcasts)} active podcasts\")\n",
    "        self.logger.info(f\"Estimated cost: ${estimated_cost:.4f}\")\n",
    "        \n",
    "        # Check cost limits\n",
    "        if not self.check_cost_limits(estimated_cost):\n",
    "            self.logger.error(\"Processing would exceed cost limits - aborting\")\n",
    "            return {'error': 'Cost limits exceeded', 'processed_episodes': 0}\n",
    "        \n",
    "        results = {\n",
    "            'processed_episodes': 0,\n",
    "            'failed_episodes': 0,\n",
    "            'skipped_episodes': 0,\n",
    "            'total_cost': 0.0,\n",
    "            'total_processing_time': 0.0,\n",
    "            'podcasts_processed': [],\n",
    "            'podcasts_failed': [],\n",
    "            'episode_details': [],\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        for podcast_name, podcast_config in active_podcasts.items():\n",
    "            podcast_start_time = datetime.now()\n",
    "            \n",
    "            try:\n",
    "                self.logger.info(f\"Processing {podcast_name} (priority: {podcast_config.get('priority', 'medium')})\")\n",
    "                \n",
    "                # Fetch recent episodes with enhanced URL extraction\n",
    "                episodes = self.fetch_recent_episodes(podcast_name, podcast_config)\n",
    "                \n",
    "                if not episodes:\n",
    "                    self.logger.info(f\"No new episodes found for {podcast_name}\")\n",
    "                    continue\n",
    "                \n",
    "                podcast_episodes_processed = 0\n",
    "                \n",
    "                for episode_data in episodes:\n",
    "                    try:\n",
    "                        # Check if episode already exists and is processed\n",
    "                        episode_id = self.save_episode(episode_data)\n",
    "                        \n",
    "                        if episode_id and episode_data.get('description'):\n",
    "                            # Check if already processed\n",
    "                            conn = sqlite3.connect(self.db_path)\n",
    "                            cursor = conn.cursor()\n",
    "                            cursor.execute('SELECT processed FROM episodes WHERE id = ?', (episode_id,))\n",
    "                            already_processed = cursor.fetchone()[0]\n",
    "                            conn.close()\n",
    "                            \n",
    "                            if already_processed:\n",
    "                                self.logger.debug(f\"Episode already processed: {episode_data['title']}\")\n",
    "                                results['skipped_episodes'] += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract intelligence with enhanced prompting\n",
    "                            intelligence = self.extract_intelligence(\n",
    "                                episode_data['description'],\n",
    "                                podcast_name,\n",
    "                                episode_data['title'],\n",
    "                                podcast_config\n",
    "                            )\n",
    "                            \n",
    "                            if intelligence:\n",
    "                                # Add episode URL to intelligence data\n",
    "                                intelligence['episode_url'] = episode_data.get('episode_url', '')\n",
    "                                \n",
    "                                self.save_intelligence(episode_id, intelligence)\n",
    "                                results['processed_episodes'] += 1\n",
    "                                podcast_episodes_processed += 1\n",
    "                                results['total_cost'] += intelligence.get('processing_cost', 0)\n",
    "                                results['total_processing_time'] += intelligence.get('processing_time_seconds', 0)\n",
    "                                \n",
    "                                # Store episode details for reporting\n",
    "                                results['episode_details'].append({\n",
    "                                    'podcast': podcast_name,\n",
    "                                    'title': episode_data['title'],\n",
    "                                    'summary': intelligence.get('executive_summary', ''),\n",
    "                                    'headline': intelligence.get('headline_takeaway', ''),\n",
    "                                    'importance': intelligence.get('importance_score', 5),\n",
    "                                    'confidence': intelligence.get('confidence_score', 0.5),\n",
    "                                    'cost': intelligence.get('processing_cost', 0),\n",
    "                                    'pub_date': episode_data['pub_date'],\n",
    "                                    'episode_url': episode_data.get('episode_url')\n",
    "                                })\n",
    "                                \n",
    "                                self.logger.info(f\"‚úÖ Processed: {episode_data['title'][:50]}...\")\n",
    "                                \n",
    "                            else:\n",
    "                                results['failed_episodes'] += 1\n",
    "                                self.logger.warning(f\"Failed to extract intelligence from: {episode_data['title']}\")\n",
    "                        \n",
    "                        else:\n",
    "                            results['skipped_episodes'] += 1\n",
    "                            self.logger.debug(f\"Skipped episode (no description): {episode_data['title']}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        results['failed_episodes'] += 1\n",
    "                        error_msg = f\"Error processing episode {episode_data.get('title', 'unknown')}: {e}\"\n",
    "                        self.logger.error(error_msg)\n",
    "                        results['errors'].append(error_msg)\n",
    "                \n",
    "                if podcast_episodes_processed > 0:\n",
    "                    results['podcasts_processed'].append(podcast_name)\n",
    "                    podcast_time = (datetime.now() - podcast_start_time).total_seconds()\n",
    "                    self.logger.info(f\"‚úÖ {podcast_name}: {podcast_episodes_processed} episodes processed in {podcast_time:.1f}s\")\n",
    "                else:\n",
    "                    self.logger.info(f\"‚ö†Ô∏è {podcast_name}: No episodes processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results['podcasts_failed'].append(podcast_name)\n",
    "                error_msg = f\"Error processing podcast {podcast_name}: {e}\"\n",
    "                self.logger.error(error_msg)\n",
    "                results['errors'].append(error_msg)\n",
    "        \n",
    "        # Calculate total processing time\n",
    "        total_time = (datetime.now() - start_time).total_seconds()\n",
    "        results['batch_processing_time'] = total_time\n",
    "        \n",
    "        # Log batch summary\n",
    "        self.logger.info(\"=== Enhanced podcast processing batch complete ===\")\n",
    "        self.logger.info(f\"Total time: {total_time:.1f}s\")\n",
    "        self.logger.info(f\"Episodes processed: {results['processed_episodes']}\")\n",
    "        self.logger.info(f\"Episodes failed: {results['failed_episodes']}\")\n",
    "        self.logger.info(f\"Episodes skipped: {results['skipped_episodes']}\")\n",
    "        self.logger.info(f\"Total cost: ${results['total_cost']:.4f}\")\n",
    "        self.logger.info(f\"Podcasts successful: {len(results['podcasts_processed'])}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_enhanced_report(self) -> Tuple[str, str]:\n",
    "        \"\"\"Generate enhanced executive-level intelligence report\"\"\"\n",
    "        self.logger.info(\"Generating enhanced intelligence reports...\")\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get recent episodes with enhanced fields\n",
    "        days_back = 7\n",
    "        since_date = (datetime.now() - timedelta(days=days_back)).isoformat()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT e.podcast_name, e.title, e.pub_date, e.duration_minutes, e.episode_url,\n",
    "                   i.headline_takeaway, i.executive_summary, i.strategic_implications,\n",
    "                   i.technical_developments, i.market_dynamics, i.key_people, \n",
    "                   i.companies_mentioned, i.predictions, i.actionable_insights,\n",
    "                   i.importance_score, i.confidence_score, i.processing_cost, i.processed_at\n",
    "            FROM episodes e\n",
    "            JOIN intelligence i ON e.id = i.episode_id\n",
    "            WHERE i.processed_at >= ?\n",
    "            ORDER BY i.importance_score DESC, i.processed_at DESC\n",
    "        ''', (since_date,))\n",
    "        \n",
    "        episodes = cursor.fetchall()\n",
    "        \n",
    "        # Get statistics\n",
    "        cursor.execute('''\n",
    "            SELECT \n",
    "                COUNT(*) as episode_count,\n",
    "                SUM(processing_cost) as total_cost,\n",
    "                AVG(importance_score) as avg_importance,\n",
    "                AVG(confidence_score) as avg_confidence,\n",
    "                COUNT(DISTINCT podcast_name) as podcast_count,\n",
    "                SUM(CASE WHEN importance_score >= 8 THEN 1 ELSE 0 END) as high_impact_count\n",
    "            FROM episodes e\n",
    "            JOIN intelligence i ON e.id = i.episode_id\n",
    "            WHERE i.processed_at >= ?\n",
    "        ''', (since_date,))\n",
    "        \n",
    "        stats = cursor.fetchone()\n",
    "        conn.close()\n",
    "        \n",
    "        if not episodes:\n",
    "            no_data_msg = f\"No episodes processed in the last {days_back} days\"\n",
    "            return no_data_msg, no_data_msg\n",
    "        \n",
    "        # Process data for report\n",
    "        all_companies = []\n",
    "        all_people = []\n",
    "        all_insights = []\n",
    "        headlines = []\n",
    "        \n",
    "        for row in episodes:\n",
    "            try:\n",
    "                # Extract headlines\n",
    "                if row[5]:  # headline_takeaway\n",
    "                    headlines.append(row[5])\n",
    "                \n",
    "                # Parse JSON fields\n",
    "                if row[10]: all_people.extend(json.loads(row[10]))\n",
    "                if row[11]: all_companies.extend(json.loads(row[11]))\n",
    "                if row[13]: all_insights.extend(json.loads(row[13]))\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # Get top items\n",
    "        from collections import Counter\n",
    "        top_companies = Counter(all_companies).most_common(8)\n",
    "        top_people = Counter(all_people).most_common(8)\n",
    "        top_insights = list(set(all_insights))[:6]\n",
    "        \n",
    "        # Generate report metadata\n",
    "        report_date = datetime.now().strftime(\"%B %d, %Y at %I:%M %p\")\n",
    "        week_range = f\"{(datetime.now() - timedelta(days=days_back)).strftime('%B %d')} - {datetime.now().strftime('%B %d, %Y')}\"\n",
    "        \n",
    "        # Create enhanced HTML report\n",
    "        html_report = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>AI Intelligence Executive Brief - {report_date}</title>\n",
    "            <style>\n",
    "                body {{\n",
    "                    font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, sans-serif;\n",
    "                    line-height: 1.6; color: #2c3e50; max-width: 1200px; margin: 0 auto;\n",
    "                    padding: 20px; background-color: #f8f9fa;\n",
    "                }}\n",
    "                .header {{\n",
    "                    background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);\n",
    "                    color: white; padding: 40px 30px; border-radius: 15px; text-align: center;\n",
    "                    margin-bottom: 30px; box-shadow: 0 10px 30px rgba(0,0,0,0.15);\n",
    "                }}\n",
    "                .executive-summary {{\n",
    "                    background: white; border-left: 6px solid #e74c3c; padding: 30px;\n",
    "                    margin: 30px 0; border-radius: 10px; box-shadow: 0 6px 20px rgba(0,0,0,0.1);\n",
    "                }}\n",
    "                .stats-grid {{\n",
    "                    display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));\n",
    "                    gap: 20px; margin: 30px 0;\n",
    "                }}\n",
    "                .stat-card {{\n",
    "                    background: white; padding: 25px; border-radius: 12px; text-align: center;\n",
    "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1); border-top: 4px solid #3498db;\n",
    "                }}\n",
    "                .section {{\n",
    "                    background: white; margin: 30px 0; padding: 35px; border-radius: 12px;\n",
    "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "                }}\n",
    "                .episode-card {{\n",
    "                    background: #f8f9fa; margin: 25px 0; padding: 30px; border-radius: 12px;\n",
    "                    border-left: 6px solid #3498db;\n",
    "                }}\n",
    "                .tag {{\n",
    "                    background: linear-gradient(135deg, #3498db, #2980b9); color: white;\n",
    "                    padding: 8px 14px; border-radius: 20px; font-size: 0.9em; margin: 3px;\n",
    "                    display: inline-block;\n",
    "                }}\n",
    "                .episode-link {{ margin-top: 15px; padding-top: 15px; border-top: 1px solid #ecf0f1; }}\n",
    "                .episode-link a {{ color: #3498db; text-decoration: none; font-weight: 500; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>AI Intelligence Executive Brief</h1>\n",
    "                <p>Week of {week_range}</p>\n",
    "                <p>Generated on {report_date}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"executive-summary\">\n",
    "                <h2>Executive Summary</h2>\n",
    "                <p><strong>Bottom line:</strong> This week's AI intelligence reveals {stats[5] or 0} high-impact developments across {stats[4] or 0} key podcasts.</p>\n",
    "                <h3>This Week's Headlines</h3>\n",
    "                <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        for headline in headlines[:5]:\n",
    "            if headline:\n",
    "                html_report += f\"<li><strong>{headline}</strong></li>\\n\"\n",
    "        \n",
    "        html_report += f\"\"\"\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"stats-grid\">\n",
    "                <div class=\"stat-card\"><h3>{stats[0] or 0}</h3><p>Episodes</p></div>\n",
    "                <div class=\"stat-card\"><h3>{stats[5] or 0}</h3><p>High-Impact</p></div>\n",
    "                <div class=\"stat-card\"><h3>${stats[1] or 0:.3f}</h3><p>Cost</p></div>\n",
    "                <div class=\"stat-card\"><h3>{stats[2] or 0:.1f}/10</h3><p>Avg Importance</p></div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Companies Mentioned</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        for company, count in top_companies:\n",
    "            if company:\n",
    "                html_report += f'<span class=\"tag\">{company} ({count})</span>\\n'\n",
    "        \n",
    "        html_report += f\"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>Episode Intelligence</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, episode in enumerate(episodes[:10], 1):\n",
    "            (podcast_name, title, pub_date, duration, episode_url, headline, summary,\n",
    "             _, _, _, _, _, _, _, importance, confidence, cost, _) = episode\n",
    "            \n",
    "            html_report += f\"\"\"\n",
    "            <div class=\"episode-card\">\n",
    "                <h3>{i}. {podcast_name}</h3>\n",
    "                <h4>{title}</h4>\n",
    "                <p><strong>Headline:</strong> {headline}</p>\n",
    "                <p>{summary}</p>\n",
    "                <p><strong>Impact:</strong> {importance}/10 | <strong>Cost:</strong> ${cost:.4f}</p>\n",
    "                {f'<div class=\"episode-link\"><a href=\"{episode_url}\" target=\"_blank\">Listen to Episode</a></div>' if episode_url else ''}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_report += \"\"\"\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Simple markdown version\n",
    "        markdown_report = f\"\"\"# AI Intelligence Executive Brief\n",
    "## Week of {week_range}\n",
    "\n",
    "### Statistics\n",
    "- Episodes: {stats[0] or 0}\n",
    "- High-Impact: {stats[5] or 0}\n",
    "- Cost: ${stats[1] or 0:.4f}\n",
    "\n",
    "### Headlines\n",
    "\"\"\"\n",
    "        for headline in headlines[:5]:\n",
    "            if headline:\n",
    "                markdown_report += f\"- {headline}\\n\"\n",
    "        \n",
    "        return html_report, markdown_report\n",
    "    \n",
    "    def save_and_display_reports(self, html_report: str, markdown_report: str) -> Tuple[str, str]:\n",
    "        \"\"\"Save reports to files\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        html_filename = self.reports_dir / f\"AI_Intelligence_Report_{timestamp}.html\"\n",
    "        markdown_filename = self.reports_dir / f\"AI_Intelligence_Report_{timestamp}.md\"\n",
    "        \n",
    "        with open(html_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        with open(markdown_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_report)\n",
    "        \n",
    "        self.logger.info(f\"Reports saved: {html_filename.name}\")\n",
    "        return str(html_filename), str(markdown_filename)\n",
    "    \n",
    "    def get_system_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive system statistics\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM episodes')\n",
    "        total_episodes = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM intelligence')\n",
    "        processed_episodes = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COALESCE(SUM(processing_cost), 0) FROM intelligence')\n",
    "        total_cost = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COALESCE(AVG(importance_score), 0) FROM intelligence')\n",
    "        avg_importance = cursor.fetchone()[0]\n",
    "        \n",
    "        week_ago = (datetime.now() - timedelta(days=7)).isoformat()\n",
    "        cursor.execute('SELECT COUNT(*) FROM intelligence WHERE processed_at >= ?', (week_ago,))\n",
    "        recent_processed = cursor.fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            'total_episodes': total_episodes,\n",
    "            'processed_episodes': processed_episodes,\n",
    "            'processing_rate': f\"{(processed_episodes/max(total_episodes, 1)*100):.1f}%\",\n",
    "            'total_cost': total_cost,\n",
    "            'avg_importance': avg_importance,\n",
    "            'recent_processed': recent_processed\n",
    "        }\n",
    "\n",
    "# Initialize the production system\n",
    "if config and anthropic_api_key:\n",
    "    print(\"\\nüîß INITIALIZING PRODUCTION SYSTEM...\")\n",
    "    system = ProductionPodcastIntelligence(config)\n",
    "    print(\"‚úÖ Production system ready!\")\n",
    "    \n",
    "    stats = system.get_system_statistics()\n",
    "    print(f\"\\nüìä SYSTEM OVERVIEW:\")\n",
    "    print(f\"üìª Total episodes: {stats['total_episodes']}\")\n",
    "    print(f\"üß† Processed: {stats['processed_episodes']} ({stats['processing_rate']})\")\n",
    "    print(f\"üí∞ Total cost: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"‚≠ê Avg importance: {stats['avg_importance']:.1f}/10\")\n",
    "    print(f\"üìà Recent: {stats['recent_processed']} episodes this week\")\n",
    "else:\n",
    "    print(\"‚ùå System initialization failed - check configuration and API key\")\n",
    "    system = None\n",
    "\n",
    "    \n",
    "# Apply the fix\n",
    "system.save_intelligence = save_intelligence_no_duplicates.__get__(system, ProductionPodcastIntelligence)\n",
    "print(\"‚úÖ Enhanced save_intelligence method applied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6887596-5bd2-4adc-a2e0-a5f5e8324b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Confidence scoring system added!\n"
     ]
    }
   ],
   "source": [
    "# CELL 3.5: Enhanced Methods Update\n",
    "# Update existing methods with enhanced functionality\n",
    "\n",
    "# Replace the extract_intelligence method\n",
    "def enhanced_extract_intelligence(self, transcript: str, podcast_name: str, episode_title: str, podcast_config: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Extract AI intelligence with enhanced executive-focused prompting\"\"\"\n",
    "    if not self.anthropic_api_key or not transcript:\n",
    "        self.logger.warning(f\"Missing API key or transcript for {episode_title}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        focus = podcast_config.get('focus', 'general')\n",
    "        priority = podcast_config.get('priority', 'medium')\n",
    "        \n",
    "        # Get focus-specific extraction guidance\n",
    "        focus_config = self.config.get('focus_areas', {}).get(focus, {})\n",
    "        extraction_emphasis = focus_config.get('extraction_emphasis', \"Focus on key insights and actionable information\")\n",
    "        \n",
    "        # ENHANCED PROMPT with executive communication principles\n",
    "        prompt = f\"\"\"\n",
    "        As a senior AI industry analyst, analyze this {podcast_name} episode and create an executive-level intelligence brief.\n",
    "        \n",
    "        PODCAST CONTEXT:\n",
    "        - Name: {podcast_name}\n",
    "        - Focus Area: {focus}\n",
    "        - Extraction Emphasis: {extraction_emphasis}\n",
    "        - Episode: {episode_title}\n",
    "        \n",
    "        TRANSCRIPT CONTENT:\n",
    "        {transcript[:4500]}\n",
    "        \n",
    "        Create a comprehensive analysis following executive communication best practices:\n",
    "        \n",
    "        1. START WITH HEADLINE: Lead with the most important takeaway\n",
    "        2. DESIGN FOR SKIMMING: Use clear structure and bold key points  \n",
    "        3. ANSWER OBJECTIONS: Address potential concerns upfront\n",
    "        4. SHOW IMPACT: Quantify business implications where possible\n",
    "        \n",
    "        Provide your analysis in this exact JSON format (NO markdown, NO code blocks, just clean JSON):\n",
    "        {{\n",
    "            \"headline_takeaway\": \"One powerful sentence capturing the most important insight from this episode\",\n",
    "            \"executive_summary\": \"A comprehensive 4-6 sentence summary that starts with the headline version, then provides crucial context. Focus on business impact, strategic implications, and actionable insights. Write for executives who scan quickly - each sentence should add clear value.\",\n",
    "            \"strategic_implications\": [\"3-4 high-level business or industry implications that executives should understand\"],\n",
    "            \"technical_developments\": [\"Specific technical advances, tools, frameworks, or breakthroughs mentioned with business context\"],\n",
    "            \"market_dynamics\": [\"Business trends, competitive insights, market shifts, or economic implications discussed\"],\n",
    "            \"key_people\": [\"Notable people mentioned in format: 'Name (Role at Company) - Key contribution or quote'\"],\n",
    "            \"companies_mentioned\": [\"Companies discussed with specific context: 'Company Name - What was said about them'\"],\n",
    "            \"predictions\": [\"Future predictions with timelines and confidence indicators where mentioned\"],\n",
    "            \"actionable_insights\": [\"4-6 specific, implementable takeaways that listeners can act on\"],\n",
    "            \"risk_factors\": [\"Potential challenges, risks, or concerns mentioned that could impact strategy\"],\n",
    "            \"quantified_impact\": [\"Any specific numbers, percentages, timelines, or measurable outcomes mentioned\"],\n",
    "            \"bottom_line\": \"One sentence that captures the core message: what should executives remember from this episode?\",\n",
    "            \"importance_score\": 8,\n",
    "        \n",
    "            \"episode_url\": \"\",\n",
    "            \"guest_expertise\": \"Brief description of the main speaker's background and why their perspective matters\"\n",
    "        }}\n",
    "        \n",
    "        CRITICAL REQUIREMENTS:\n",
    "        - Executive summary must be 4-6 substantive sentences minimum\n",
    "        - Focus on business impact and strategic value\n",
    "        - Use clear, decisive language\n",
    "        - Quantify impact where possible\n",
    "        - Address potential objections or concerns\n",
    "        - Return clean JSON only - no formatting, no code blocks, no extra text\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Call Claude API with enhanced settings\n",
    "        response = requests.post(\n",
    "            \"https://api.anthropic.com/v1/messages\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"x-api-key\": self.anthropic_api_key,\n",
    "                \"anthropic-version\": \"2023-06-01\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"claude-sonnet-4-20250514\",\n",
    "                \"max_tokens\": 3500,  # Increased for longer summaries\n",
    "                \"temperature\": 0.2,   # Lower for more focused output\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            claude_response = result['content'][0]['text'].strip()\n",
    "            \n",
    "            # Clean up any markdown formatting that might appear\n",
    "            if claude_response.startswith('```json'):\n",
    "                claude_response = claude_response.replace('```json', '').replace('```', '').strip()\n",
    "            if claude_response.startswith('```'):\n",
    "                claude_response = claude_response.replace('```', '').strip()\n",
    "            \n",
    "            # Calculate processing cost\n",
    "            input_tokens = len(prompt) / 4\n",
    "            output_tokens = len(claude_response) / 4\n",
    "            cost = (input_tokens * 3 + output_tokens * 15) / 1_000_000\n",
    "            \n",
    "            try:\n",
    "                # Parse JSON response\n",
    "                intelligence_data = json.loads(claude_response)\n",
    "                \n",
    "                # Add metadata\n",
    "                intelligence_data.update({\n",
    "                    'processing_cost': cost,\n",
    "                    'processing_time_seconds': processing_time,\n",
    "                    'model_used': 'claude-sonnet-4-20250514',\n",
    "                    'extraction_timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                # Calculate real confidence score\n",
    "                intelligence_data['confidence_score'] = self.calculate_confidence_score(transcript, intelligence_data, processing_time),\n",
    "                \n",
    "                self.logger.info(f\"Successfully extracted enhanced intelligence from {episode_title} (${cost:.4f}, {processing_time:.1f}s)\")\n",
    "                self.log_operation('extract_intelligence', podcast_name, episode_title, 'success', cost=cost, processing_time=processing_time)\n",
    "                \n",
    "                return intelligence_data\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                self.logger.warning(f\"Failed to parse JSON response for {episode_title}: {e}\")\n",
    "                self.logger.debug(f\"Raw response: {claude_response[:200]}...\")\n",
    "                \n",
    "                # Fallback response with executive summary\n",
    "                fallback_data = {\n",
    "                    'headline_takeaway': f\"Analysis of {episode_title} from {podcast_name}\",\n",
    "                    'executive_summary': claude_response[:800] + \"...\" if len(claude_response) > 800 else claude_response,\n",
    "                    'processing_cost': cost,\n",
    "                    'processing_time_seconds': processing_time,\n",
    "                    'model_used': 'claude-sonnet-4-20250514',\n",
    "                    'importance_score': 5,\n",
    "                    'confidence_score': 0.5,\n",
    "                    'bottom_line': 'Raw analysis available in executive summary due to parsing issue',\n",
    "                    'parsing_error': True\n",
    "                }\n",
    "                \n",
    "                return fallback_data\n",
    "                \n",
    "        else:\n",
    "            error_msg = f\"Claude API error: {response.status_code} - {response.text}\"\n",
    "            self.logger.error(error_msg)\n",
    "            self.log_operation('extract_intelligence', podcast_name, episode_title, 'error', error_message=error_msg)\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Intelligence extraction error: {e}\"\n",
    "        self.logger.error(error_msg)\n",
    "        self.log_operation('extract_intelligence', podcast_name, episode_title, 'error', error_message=error_msg)\n",
    "        return None\n",
    "\n",
    "# Replace the save_intelligence method  \n",
    "def enhanced_save_intelligence(self, episode_id: int, intelligence_data: Dict):\n",
    "    \"\"\"Save extracted intelligence with enhanced fields\"\"\"\n",
    "    conn = sqlite3.connect(self.db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # First, update the episodes table with episode URL if available\n",
    "        if intelligence_data.get('episode_url'):\n",
    "            cursor.execute(\n",
    "                'UPDATE episodes SET episode_url = ? WHERE id = ?',\n",
    "                (intelligence_data.get('episode_url'), episode_id)\n",
    "            )\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO intelligence (\n",
    "                episode_id, executive_summary, technical_developments,\n",
    "                industry_business, key_people, companies_mentioned,\n",
    "                predictions, key_insights, actionable_items,\n",
    "                importance_score, confidence_score, processing_cost,\n",
    "                processing_time_seconds, model_used\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            episode_id,\n",
    "            # Store the enhanced executive summary\n",
    "            f\"{intelligence_data.get('headline_takeaway', '')} | {intelligence_data.get('executive_summary', '')}\",\n",
    "            json.dumps(intelligence_data.get('technical_developments', [])),\n",
    "            json.dumps(intelligence_data.get('market_dynamics', intelligence_data.get('industry_business', []))),\n",
    "            json.dumps(intelligence_data.get('key_people', [])),\n",
    "            json.dumps(intelligence_data.get('companies_mentioned', [])),\n",
    "            json.dumps(intelligence_data.get('predictions', [])),\n",
    "            json.dumps(intelligence_data.get('actionable_insights', intelligence_data.get('key_insights', []))),\n",
    "            json.dumps(intelligence_data.get('strategic_implications', [])),\n",
    "            int(intelligence_data.get('importance_score', 5)),\n",
    "            float(intelligence_data.get('confidence_score', 0.5)),\n",
    "            intelligence_data.get('processing_cost', 0.0),\n",
    "            intelligence_data.get('processing_time_seconds', 0.0),\n",
    "            intelligence_data.get('model_used', 'unknown')\n",
    "        ))\n",
    "        \n",
    "        # Mark episode as processed\n",
    "        cursor.execute('UPDATE episodes SET processed = 1, updated_at = CURRENT_TIMESTAMP WHERE id = ?', (episode_id,))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        self.logger.debug(f\"Enhanced intelligence saved for episode ID: {episode_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error saving intelligence: {e}\")\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# Replace the generate_production_report method\n",
    "def enhanced_generate_report(self) -> Tuple[str, str]:\n",
    "    \"\"\"Generate executive-level intelligence report with enhanced formatting\"\"\"\n",
    "    self.logger.info(\"Generating enhanced intelligence reports...\")\n",
    "    \n",
    "    conn = sqlite3.connect(self.db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get recent episodes with episode URLs\n",
    "    days_back = 7\n",
    "    since_date = (datetime.now() - timedelta(days=days_back)).isoformat()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT e.podcast_name, e.title, e.pub_date, e.duration_minutes, e.episode_url,\n",
    "               i.executive_summary, i.technical_developments, i.industry_business,\n",
    "               i.key_people, i.companies_mentioned, i.predictions, i.key_insights,\n",
    "               i.actionable_items, i.importance_score, i.confidence_score,\n",
    "               i.processing_cost, i.processed_at\n",
    "        FROM episodes e\n",
    "        JOIN intelligence i ON e.id = i.episode_id\n",
    "        WHERE i.processed_at >= ?\n",
    "        ORDER BY i.importance_score DESC, i.processed_at DESC\n",
    "    ''', (since_date,))\n",
    "    \n",
    "    episodes = cursor.fetchall()\n",
    "    \n",
    "    # Get aggregated statistics\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) as episode_count,\n",
    "            SUM(processing_cost) as total_cost,\n",
    "            AVG(importance_score) as avg_importance,\n",
    "            AVG(confidence_score) as avg_confidence,\n",
    "            COUNT(DISTINCT podcast_name) as podcast_count,\n",
    "            SUM(CASE WHEN importance_score >= 8 THEN 1 ELSE 0 END) as high_impact_count\n",
    "        FROM episodes e\n",
    "        JOIN intelligence i ON e.id = i.episode_id\n",
    "        WHERE i.processed_at >= ?\n",
    "    ''', (since_date,))\n",
    "    \n",
    "    stats = cursor.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if not episodes:\n",
    "        no_data_msg = f\"üì≠ No episodes processed in the last {days_back} days\"\n",
    "        return no_data_msg, no_data_msg\n",
    "    \n",
    "    # Process aggregated data with enhanced structure\n",
    "    all_companies = []\n",
    "    all_people = []\n",
    "    all_insights = []\n",
    "    all_strategic = []\n",
    "    \n",
    "    for row in episodes:\n",
    "        try:\n",
    "            # Parse JSON fields\n",
    "            companies = json.loads(row[9]) if row[9] else []\n",
    "            people = json.loads(row[8]) if row[8] else []\n",
    "            insights = json.loads(row[11]) if row[11] else []\n",
    "            strategic = json.loads(row[12]) if row[12] else []  # actionable_items\n",
    "            \n",
    "            all_companies.extend(companies)\n",
    "            all_people.extend(people)\n",
    "            all_insights.extend(insights)\n",
    "            all_strategic.extend(strategic)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # Get top items\n",
    "    from collections import Counter\n",
    "    top_companies = Counter(all_companies).most_common(8)\n",
    "    top_people = Counter(all_people).most_common(8)\n",
    "    top_insights = list(set(all_insights))[:5]\n",
    "    top_strategic = list(set(all_strategic))[:6]\n",
    "    \n",
    "    # Generate report metadata\n",
    "    report_date = datetime.now().strftime(\"%B %d, %Y at %I:%M %p\")\n",
    "    week_range = f\"{(datetime.now() - timedelta(days=days_back)).strftime('%B %d')} - {datetime.now().strftime('%B %d, %Y')}\"\n",
    "    \n",
    "    # Extract top headline takeaways\n",
    "    headlines = []\n",
    "    for row in episodes[:5]:  # Top 5 most important episodes\n",
    "        summary = row[5]  # executive_summary\n",
    "        if '|' in summary:\n",
    "            headline = summary.split('|')[0].strip()\n",
    "            headlines.append(headline)\n",
    "        else:\n",
    "            headlines.append(summary[:100] + \"...\" if len(summary) > 100 else summary)\n",
    "    \n",
    "    # Generate Enhanced HTML Report\n",
    "    html_report = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>AI Intelligence Executive Brief - {report_date}</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, sans-serif;\n",
    "                line-height: 1.6;\n",
    "                color: #2c3e50;\n",
    "                max-width: 1200px;\n",
    "                margin: 0 auto;\n",
    "                padding: 20px;\n",
    "                background-color: #f8f9fa;\n",
    "            }}\n",
    "            \n",
    "            .header {{\n",
    "                background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);\n",
    "                color: white;\n",
    "                padding: 40px 30px;\n",
    "                border-radius: 15px;\n",
    "                text-align: center;\n",
    "                margin-bottom: 30px;\n",
    "                box-shadow: 0 10px 30px rgba(0,0,0,0.15);\n",
    "            }}\n",
    "            \n",
    "            .header h1 {{\n",
    "                margin: 0;\n",
    "                font-size: 2.5em;\n",
    "                font-weight: 300;\n",
    "                letter-spacing: -1px;\n",
    "            }}\n",
    "            \n",
    "            .header .subtitle {{\n",
    "                margin: 15px 0 5px 0;\n",
    "                opacity: 0.9;\n",
    "                font-size: 1.2em;\n",
    "                font-weight: 500;\n",
    "            }}\n",
    "            \n",
    "            .header .tagline {{\n",
    "                margin: 5px 0 0 0;\n",
    "                opacity: 0.8;\n",
    "                font-size: 1em;\n",
    "            }}\n",
    "            \n",
    "            .executive-summary {{\n",
    "                background: white;\n",
    "                border-left: 6px solid #e74c3c;\n",
    "                padding: 30px;\n",
    "                margin: 30px 0;\n",
    "                border-radius: 10px;\n",
    "                box-shadow: 0 6px 20px rgba(0,0,0,0.1);\n",
    "            }}\n",
    "            \n",
    "            .executive-summary h2 {{\n",
    "                margin-top: 0;\n",
    "                color: #e74c3c;\n",
    "                font-size: 1.8em;\n",
    "                font-weight: 600;\n",
    "            }}\n",
    "            \n",
    "            .headlines {{\n",
    "                list-style: none;\n",
    "                padding: 0;\n",
    "                margin: 20px 0;\n",
    "            }}\n",
    "            \n",
    "            .headlines li {{\n",
    "                background: #ecf0f1;\n",
    "                margin: 12px 0;\n",
    "                padding: 15px 20px;\n",
    "                border-radius: 8px;\n",
    "                border-left: 4px solid #3498db;\n",
    "                font-weight: 500;\n",
    "                position: relative;\n",
    "            }}\n",
    "            \n",
    "            .headlines li::before {{\n",
    "                content: \"‚ñ∂\";\n",
    "                color: #3498db;\n",
    "                font-weight: bold;\n",
    "                position: absolute;\n",
    "                left: 8px;\n",
    "            }}\n",
    "            \n",
    "            .headlines li {{\n",
    "                padding-left: 35px;\n",
    "            }}\n",
    "            \n",
    "            .stats-grid {{\n",
    "                display: grid;\n",
    "                grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));\n",
    "                gap: 20px;\n",
    "                margin: 30px 0;\n",
    "            }}\n",
    "            \n",
    "            .stat-card {{\n",
    "                background: white;\n",
    "                padding: 25px;\n",
    "                border-radius: 12px;\n",
    "                text-align: center;\n",
    "                box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "                border-top: 4px solid #3498db;\n",
    "                transition: transform 0.2s ease;\n",
    "            }}\n",
    "            \n",
    "            .stat-card:hover {{\n",
    "                transform: translateY(-3px);\n",
    "            }}\n",
    "            \n",
    "            .stat-card h3 {{\n",
    "                margin: 0 0 10px 0;\n",
    "                font-size: 2.2em;\n",
    "                color: #2c3e50;\n",
    "                font-weight: 700;\n",
    "            }}\n",
    "            \n",
    "            .stat-card p {{\n",
    "                margin: 0;\n",
    "                color: #7f8c8d;\n",
    "                font-weight: 500;\n",
    "                font-size: 0.95em;\n",
    "            }}\n",
    "            \n",
    "            .section {{\n",
    "                background: white;\n",
    "                margin: 30px 0;\n",
    "                padding: 35px;\n",
    "                border-radius: 12px;\n",
    "                box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "            }}\n",
    "            \n",
    "            .section h2 {{\n",
    "                color: #2c3e50;\n",
    "                border-bottom: 3px solid #ecf0f1;\n",
    "                padding-bottom: 15px;\n",
    "                margin-top: 0;\n",
    "                font-size: 1.8em;\n",
    "                font-weight: 600;\n",
    "            }}\n",
    "            \n",
    "            .episode-card {{\n",
    "                background: #f8f9fa;\n",
    "                margin: 25px 0;\n",
    "                padding: 30px;\n",
    "                border-radius: 12px;\n",
    "                border-left: 6px solid #3498db;\n",
    "                transition: all 0.3s ease;\n",
    "                position: relative;\n",
    "            }}\n",
    "            \n",
    "            .episode-card:hover {{\n",
    "                transform: translateY(-2px);\n",
    "                box-shadow: 0 8px 25px rgba(0,0,0,0.15);\n",
    "            }}\n",
    "            \n",
    "            .episode-header {{\n",
    "                display: flex;\n",
    "                justify-content: space-between;\n",
    "                align-items: flex-start;\n",
    "                margin-bottom: 20px;\n",
    "                flex-wrap: wrap;\n",
    "                gap: 15px;\n",
    "            }}\n",
    "            \n",
    "            .episode-title {{\n",
    "                flex: 1;\n",
    "                min-width: 300px;\n",
    "            }}\n",
    "            \n",
    "            .episode-title h3 {{\n",
    "                margin: 0 0 8px 0;\n",
    "                color: #2c3e50;\n",
    "                font-size: 1.4em;\n",
    "                font-weight: 600;\n",
    "            }}\n",
    "            \n",
    "            .episode-title h4 {{\n",
    "                margin: 0;\n",
    "                color: #34495e;\n",
    "                font-size: 1.1em;\n",
    "                font-weight: 500;\n",
    "                line-height: 1.4;\n",
    "            }}\n",
    "            \n",
    "            .episode-meta {{\n",
    "                display: flex;\n",
    "                gap: 12px;\n",
    "                flex-wrap: wrap;\n",
    "                align-items: center;\n",
    "            }}\n",
    "            \n",
    "            .badge {{\n",
    "                padding: 6px 14px;\n",
    "                border-radius: 20px;\n",
    "                font-size: 0.85em;\n",
    "                font-weight: 600;\n",
    "                color: white;\n",
    "                white-space: nowrap;\n",
    "            }}\n",
    "            \n",
    "            .importance-high {{ background: linear-gradient(135deg, #27ae60, #2ecc71); }}\n",
    "            .importance-medium {{ background: linear-gradient(135deg, #f39c12, #e67e22); }}\n",
    "            .importance-low {{ background: linear-gradient(135deg, #95a5a6, #7f8c8d); }}\n",
    "            \n",
    "            .confidence-badge {{ background: linear-gradient(135deg, #3498db, #2980b9); }}\n",
    "            .cost-badge {{ background: linear-gradient(135deg, #9b59b6, #8e44ad); }}\n",
    "            \n",
    "            .episode-summary {{\n",
    "                margin: 20px 0;\n",
    "                padding: 20px;\n",
    "                background: white;\n",
    "                border-radius: 8px;\n",
    "                border-left: 4px solid #e74c3c;\n",
    "            }}\n",
    "            \n",
    "            .episode-summary .headline {{\n",
    "                font-weight: 700;\n",
    "                color: #e74c3c;\n",
    "                font-size: 1.05em;\n",
    "                margin-bottom: 10px;\n",
    "            }}\n",
    "            \n",
    "            .episode-summary .details {{\n",
    "                color: #2c3e50;\n",
    "                line-height: 1.7;\n",
    "            }}\n",
    "            \n",
    "            .episode-link {{\n",
    "                margin-top: 15px;\n",
    "                padding-top: 15px;\n",
    "                border-top: 1px solid #ecf0f1;\n",
    "            }}\n",
    "            \n",
    "            .episode-link a {{\n",
    "                color: #3498db;\n",
    "                text-decoration: none;\n",
    "                font-weight: 500;\n",
    "                font-size: 0.95em;\n",
    "            }}\n",
    "            \n",
    "            .episode-link a:hover {{\n",
    "                text-decoration: underline;\n",
    "                color: #2980b9;\n",
    "            }}\n",
    "            \n",
    "            .insights-grid {{\n",
    "                display: grid;\n",
    "                grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));\n",
    "                gap: 25px;\n",
    "                margin: 25px 0;\n",
    "            }}\n",
    "            \n",
    "            .insight-box {{\n",
    "                background: #ecf0f1;\n",
    "                padding: 20px;\n",
    "                border-radius: 10px;\n",
    "                border-left: 4px solid #e74c3c;\n",
    "                position: relative;\n",
    "            }}\n",
    "            \n",
    "            .insight-box::before {{\n",
    "                content: \"üí°\";\n",
    "                position: absolute;\n",
    "                top: 15px;\n",
    "                left: 15px;\n",
    "                font-size: 1.2em;\n",
    "            }}\n",
    "            \n",
    "            .insight-box {{\n",
    "                padding-left: 50px;\n",
    "            }}\n",
    "            \n",
    "            .companies-people-grid {{\n",
    "                display: grid;\n",
    "                grid-template-columns: 1fr 1fr;\n",
    "                gap: 30px;\n",
    "                margin: 25px 0;\n",
    "            }}\n",
    "            \n",
    "            .tag-container {{\n",
    "                display: flex;\n",
    "                flex-wrap: wrap;\n",
    "                gap: 8px;\n",
    "                margin: 15px 0;\n",
    "            }}\n",
    "            \n",
    "            .tag {{\n",
    "                background: linear-gradient(135deg, #3498db, #2980b9);\n",
    "                color: white;\n",
    "                padding: 8px 14px;\n",
    "                border-radius: 20px;\n",
    "                font-size: 0.9em;\n",
    "                font-weight: 500;\n",
    "                transition: transform 0.2s ease;\n",
    "            }}\n",
    "            \n",
    "            .tag:hover {{\n",
    "                transform: scale(1.05);\n",
    "            }}\n",
    "            \n",
    "            .bottom-line {{\n",
    "                background: linear-gradient(135deg, #2c3e50, #34495e);\n",
    "                color: white;\n",
    "                padding: 25px 35px;\n",
    "                border-radius: 12px;\n",
    "                text-align: center;\n",
    "                margin: 40px 0;\n",
    "                box-shadow: 0 6px 20px rgba(0,0,0,0.15);\n",
    "            }}\n",
    "            \n",
    "            .bottom-line h2 {{\n",
    "                margin: 0 0 15px 0;\n",
    "                font-size: 1.6em;\n",
    "                font-weight: 600;\n",
    "            }}\n",
    "            \n",
    "            .bottom-line p {{\n",
    "                margin: 0;\n",
    "                font-size: 1.1em;\n",
    "                opacity: 0.95;\n",
    "                line-height: 1.6;\n",
    "            }}\n",
    "            \n",
    "            .footer {{\n",
    "                text-align: center;\n",
    "                margin: 50px 0 20px 0;\n",
    "                padding: 30px;\n",
    "                background: white;\n",
    "                border-radius: 12px;\n",
    "                box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "                color: #7f8c8d;\n",
    "            }}\n",
    "            \n",
    "            @media (max-width: 768px) {{\n",
    "                .companies-people-grid {{\n",
    "                    grid-template-columns: 1fr;\n",
    "                }}\n",
    "                \n",
    "                .episode-header {{\n",
    "                    flex-direction: column;\n",
    "                    align-items: stretch;\n",
    "                }}\n",
    "                \n",
    "                .episode-meta {{\n",
    "                    justify-content: flex-start;\n",
    "                }}\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"header\">\n",
    "            <h1>üéØ AI Intelligence Executive Brief</h1>\n",
    "            <p class=\"subtitle\">Week of {week_range}</p>\n",
    "            <p class=\"tagline\">Strategic Intelligence ‚Ä¢ Market Insights ‚Ä¢ Technical Developments</p>\n",
    "            <p class=\"tagline\">Generated on {report_date}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"executive-summary\">\n",
    "            <h2>üìã Executive Summary</h2>\n",
    "            <p><strong>Bottom line:</strong> This week's AI intelligence reveals {stats[5] or 0} high-impact developments across {stats[4] or 0} key podcasts, with particularly strong insights in {', '.join([insight[:30] + '...' if len(insight) > 30 else insight for insight in top_insights[:2]])}.</p>\n",
    "            \n",
    "            <h3 style=\"margin: 25px 0 15px 0; color: #2c3e50;\">üéØ This Week's Headlines</h3>\n",
    "            <ul class=\"headlines\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add top headlines\n",
    "    for headline in headlines:\n",
    "        if headline:\n",
    "            html_report += f\"<li>{headline}</li>\\n\"\n",
    "    \n",
    "    html_report += f\"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"stats-grid\">\n",
    "            <div class=\"stat-card\">\n",
    "                <h3>{stats[0] or 0}</h3>\n",
    "                <p>Episodes Analyzed</p>\n",
    "            </div>\n",
    "            <div class=\"stat-card\">\n",
    "                <h3>{stats[5] or 0}</h3>\n",
    "                <p>High-Impact Stories</p>\n",
    "            </div>\n",
    "            <div class=\"stat-card\">\n",
    "                <h3>{stats[4] or 0}</h3>\n",
    "                <p>Podcast Sources</p>\n",
    "            </div>\n",
    "            <div class=\"stat-card\">\n",
    "                <h3>{stats[2] or 0:.1f}/10</h3>\n",
    "                <p>Avg Importance</p>\n",
    "            </div>\n",
    "            <div class=\"stat-card\">\n",
    "                <h3>${stats[1] or 0:.3f}</h3>\n",
    "                <p>Processing Investment</p>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>üíº Strategic Implications</h2>\n",
    "            <div class=\"insights-grid\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for insight in top_strategic:\n",
    "        if insight:\n",
    "            html_report += f'<div class=\"insight-box\">{insight}</div>\\n'\n",
    "    \n",
    "    html_report += f\"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>üè¢ Market Intelligence</h2>\n",
    "            <div class=\"companies-people-grid\">\n",
    "                <div>\n",
    "                    <h3 style=\"color: #2c3e50; margin-bottom: 15px;\">Companies in Focus</h3>\n",
    "                    <div class=\"tag-container\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for company, count in top_companies:\n",
    "        if company:\n",
    "            html_report += f'<span class=\"tag\">{company} ({count})</span>\\n'\n",
    "    \n",
    "    html_report += f\"\"\"\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div>\n",
    "                    <h3 style=\"color: #2c3e50; margin-bottom: 15px;\">Key Industry Voices</h3>\n",
    "                    <div class=\"tag-container\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for person, count in top_people:\n",
    "        if person:\n",
    "            # Clean up person format if it includes extra details\n",
    "            person_clean = person.split(' - ')[0] if ' - ' in person else person\n",
    "            html_report += f'<span class=\"tag\">{person_clean}</span>\\n'\n",
    "    \n",
    "    html_report += f\"\"\"\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>üìª Detailed Episode Intelligence</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add episode details with enhanced formatting\n",
    "    for i, episode in enumerate(episodes[:12], 1):  # Show top 12 episodes\n",
    "        (podcast_name, title, pub_date, duration, episode_url, summary, \n",
    "         tech_dev, industry, people, companies, predictions, insights, \n",
    "         actionable, importance, confidence, cost, processed_at) = episode\n",
    "        \n",
    "        # Determine importance class and text\n",
    "        if importance >= 8:\n",
    "            importance_class = \"importance-high\"\n",
    "            importance_text = \"High Impact\"\n",
    "        elif importance >= 6:\n",
    "            importance_class = \"importance-medium\"\n",
    "            importance_text = \"Medium Impact\"\n",
    "        else:\n",
    "            importance_class = \"importance-low\"\n",
    "            importance_text = \"Standard\"\n",
    "        \n",
    "        # Split headline from summary if available\n",
    "        if '|' in summary:\n",
    "            headline, details = summary.split('|', 1)\n",
    "            headline = headline.strip()\n",
    "            details = details.strip()\n",
    "        else:\n",
    "            headline = summary[:100] + \"...\" if len(summary) > 100 else summary\n",
    "            details = summary\n",
    "        \n",
    "        duration_text = f\"{duration} min\" if duration else \"Duration unknown\"\n",
    "        \n",
    "        html_report += f\"\"\"\n",
    "        <div class=\"episode-card\">\n",
    "            <div class=\"episode-header\">\n",
    "                <div class=\"episode-title\">\n",
    "                    <h3>{i}. {podcast_name}</h3>\n",
    "                    <h4>{title}</h4>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"episode-meta\">\n",
    "                    <span class=\"badge {importance_class}\">‚≠ê {importance}/10 ‚Ä¢ {importance_text}</span>\n",
    "                    <span class=\"badge confidence-badge\">üéØ {confidence:.1f} Confidence</span>\n",
    "                    <span class=\"badge cost-badge\">üí∞ ${cost:.4f}</span>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"episode-summary\">\n",
    "                <div class=\"headline\">{headline}</div>\n",
    "                <div class=\"details\">{details}</div>\n",
    "            </div>\n",
    "            \n",
    "            <p style=\"color: #7f8c8d; margin: 15px 0 0 0; font-size: 0.95em;\">\n",
    "                <strong>Published:</strong> {pub_date[:10]} ‚Ä¢ <strong>Duration:</strong> {duration_text} ‚Ä¢ <strong>Processed:</strong> {processed_at[:10]}\n",
    "            </p>\n",
    "            \n",
    "            {f'<div class=\"episode-link\"><a href=\"{episode_url}\" target=\"_blank\">üîó Listen to Episode</a></div>' if episode_url else ''}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_report += f\"\"\"\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"bottom-line\">\n",
    "            <h2>üéØ Bottom Line</h2>\n",
    "            <p>AI industry momentum continues with {stats[5] or 0} high-impact developments this week. Key focus areas: strategic AI implementation, market consolidation, and technical breakthroughs driving competitive advantage. <strong>Recommendation:</strong> Monitor developments in {top_companies[0][0] if top_companies else 'leading AI companies'} and prepare for accelerated adoption timelines.</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"footer\">\n",
    "            <p><strong>AI Intelligence Executive Brief</strong></p>\n",
    "            <p>ü§ñ Powered by Claude AI ‚Ä¢ üìä JupyterLab Production Environment ‚Ä¢ üéØ Executive Communication Optimized</p>\n",
    "            <p><em>Next briefing: {(datetime.now() + timedelta(days=7)).strftime('%B %d, %Y')}</em></p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate Enhanced Markdown Report\n",
    "    markdown_report = f\"\"\"# üéØ AI Intelligence Executive Brief\n",
    "## Week of {week_range}\n",
    "*Generated on {report_date}*\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Executive Summary\n",
    "\n",
    "**Bottom line:** This week's AI intelligence reveals {stats[5] or 0} high-impact developments across {stats[4] or 0} key podcasts.\n",
    "\n",
    "### üéØ This Week's Headlines\n",
    "\"\"\"\n",
    "    \n",
    "    for headline in headlines:\n",
    "        if headline:\n",
    "            markdown_report += f\"- **{headline}**\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Intelligence Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Episodes Analyzed | {stats[0] or 0} |\n",
    "| High-Impact Stories | {stats[5] or 0} |\n",
    "| Podcast Sources | {stats[4] or 0} |\n",
    "| Average Importance | {stats[2] or 0:.1f}/10 |\n",
    "| Processing Investment | ${stats[1] or 0:.4f} |\n",
    "| Average Confidence | {stats[3] or 0:.1f} |\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Strategic Implications\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for insight in top_strategic:\n",
    "        if insight:\n",
    "            markdown_report += f\"- **{insight}**\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## üè¢ Market Intelligence\n",
    "\n",
    "### Companies in Focus\n",
    "\"\"\"\n",
    "    \n",
    "    for company, count in top_companies[:8]:\n",
    "        if company:\n",
    "            markdown_report += f\"- **{company}** (mentioned {count} times)\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "\n",
    "### Key Industry Voices\n",
    "\"\"\"\n",
    "    \n",
    "    for person, count in top_people[:8]:\n",
    "        if person:\n",
    "            person_clean = person.split(' - ')[0] if ' - ' in person else person\n",
    "            markdown_report += f\"- **{person_clean}** (mentioned {count} times)\\n\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìª Detailed Episode Intelligence\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, episode in enumerate(episodes[:10], 1):\n",
    "        (podcast_name, title, pub_date, duration, episode_url, summary, \n",
    "         tech_dev, industry, people, companies, predictions, insights, \n",
    "         actionable, importance, confidence, cost, processed_at) = episode\n",
    "        \n",
    "        # Split headline from summary\n",
    "        if '|' in summary:\n",
    "            headline, details = summary.split('|', 1)\n",
    "            headline = headline.strip()\n",
    "            details = details.strip()\n",
    "        else:\n",
    "            headline = summary[:100] + \"...\" if len(summary) > 100 else summary\n",
    "            details = summary\n",
    "        \n",
    "        duration_text = f\"{duration} min\" if duration else \"Unknown\"\n",
    "        \n",
    "        markdown_report += f\"\"\"\n",
    "### {i}. {podcast_name} - {title}\n",
    "\n",
    "**Impact:** {importance}/10 ‚Ä¢ **Confidence:** {confidence:.1f} ‚Ä¢ **Cost:** ${cost:.4f} ‚Ä¢ **Duration:** {duration_text}\n",
    "\n",
    "**Headline:** {headline}\n",
    "\n",
    "**Analysis:** {details}\n",
    "\n",
    "{f'**Episode Link:** [{episode_url}]({episode_url})' if episode_url else ''}\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    markdown_report += f\"\"\"\n",
    "\n",
    "## üéØ Bottom Line\n",
    "\n",
    "AI industry momentum continues with {stats[5] or 0} high-impact developments this week. Key focus areas: strategic AI implementation, market consolidation, and technical breakthroughs driving competitive advantage.\n",
    "\n",
    "**Recommendation:** Monitor developments in {top_companies[0][0] if top_companies else 'leading AI companies'} and prepare for accelerated adoption timelines.\n",
    "\n",
    "---\n",
    "\n",
    "*Next briefing: {(datetime.now() + timedelta(days=7)).strftime('%B %d, %Y')}*\n",
    "\n",
    "*ü§ñ Powered by Claude AI & Lily ‚Ä¢ üìä JupyterLab Production Environment ‚Ä¢ üéØ Executive Communication Optimized*\n",
    "\"\"\"\n",
    "    \n",
    "    self.logger.info(\"Enhanced executive reports generated successfully\")\n",
    "    return html_report, markdown_report\n",
    "\n",
    "\n",
    "# Add new database schema update method\n",
    "def update_database_schema(self):\n",
    "    \"\"\"Update database schema to include episode URLs\"\"\"\n",
    "    conn = sqlite3.connect(self.db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Check if episode_url column exists\n",
    "        cursor.execute(\"PRAGMA table_info(episodes)\")\n",
    "        columns = [column[1] for column in cursor.fetchall()]\n",
    "        \n",
    "        if 'episode_url' not in columns:\n",
    "            cursor.execute('ALTER TABLE episodes ADD COLUMN episode_url TEXT')\n",
    "            self.logger.info(\"Added episode_url column to episodes table\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error updating database schema: {e}\")\n",
    "        conn.close()\n",
    "\n",
    "def calculate_confidence_score(self, transcript: str, intelligence_data: Dict, processing_time: float) -> float:\n",
    "    \"\"\"Calculate actual confidence score based on analysis quality\"\"\"\n",
    "    \n",
    "    confidence_factors = []\n",
    "    \n",
    "    # 1. SOURCE QUALITY (0-0.3 points)\n",
    "    source_quality = self._assess_source_quality(transcript)\n",
    "    confidence_factors.append((\"source_quality\", source_quality, 0.3))\n",
    "    \n",
    "    # 2. CONTENT COMPLETENESS (0-0.3 points)\n",
    "    completeness = self._assess_content_completeness(intelligence_data)\n",
    "    confidence_factors.append((\"completeness\", completeness, 0.3))\n",
    "    \n",
    "    # 3. ANALYSIS DEPTH (0-0.25 points)\n",
    "    depth = self._assess_analysis_depth(intelligence_data)\n",
    "    confidence_factors.append((\"depth\", depth, 0.25))\n",
    "    \n",
    "    # 4. PROCESSING INDICATORS (0-0.15 points)\n",
    "    processing_quality = self._assess_processing_quality(processing_time, intelligence_data)\n",
    "    confidence_factors.append((\"processing\", processing_quality, 0.15))\n",
    "    \n",
    "    # Calculate weighted score\n",
    "    total_score = sum(factor_score * weight for _, factor_score, weight in confidence_factors)\n",
    "    \n",
    "    # Log confidence breakdown for debugging\n",
    "    self.logger.debug(f\"Confidence breakdown: {confidence_factors} = {total_score:.3f}\")\n",
    "    \n",
    "    return round(total_score, 3)\n",
    "\n",
    "def _assess_source_quality(self, transcript: str) -> float:\n",
    "    \"\"\"Assess quality of source material (0-1)\"\"\"\n",
    "    if not transcript:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # Length check (more content = potentially better analysis)\n",
    "    if len(transcript) > 2000:\n",
    "        score += 0.4\n",
    "    elif len(transcript) > 1000:\n",
    "        score += 0.3\n",
    "    elif len(transcript) > 500:\n",
    "        score += 0.2\n",
    "    else:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Content indicators (technical terms, business language)\n",
    "    business_terms = ['strategy', 'market', 'revenue', 'growth', 'competitive', 'innovation', 'industry']\n",
    "    technical_terms = ['AI', 'machine learning', 'algorithm', 'model', 'data', 'training', 'inference']\n",
    "    \n",
    "    business_mentions = sum(1 for term in business_terms if term.lower() in transcript.lower())\n",
    "    technical_mentions = sum(1 for term in technical_terms if term.lower() in transcript.lower())\n",
    "    \n",
    "    # Bonus for relevant terminology\n",
    "    if business_mentions >= 3:\n",
    "        score += 0.2\n",
    "    elif business_mentions >= 1:\n",
    "        score += 0.1\n",
    "    \n",
    "    if technical_mentions >= 3:\n",
    "        score += 0.2\n",
    "    elif technical_mentions >= 1:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Content structure indicators\n",
    "    if '?' in transcript:  # Questions indicate dialogue/interview\n",
    "        score += 0.1\n",
    "    if any(indicator in transcript.lower() for indicator in ['discuss', 'explain', 'talk about']):\n",
    "        score += 0.1\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "def _assess_content_completeness(self, intelligence_data: Dict) -> float:\n",
    "    \"\"\"Assess how complete the extracted intelligence is (0-1)\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Required fields quality\n",
    "    required_fields = {\n",
    "        'headline_takeaway': (50, 0.15),  # min length, points\n",
    "        'executive_summary': (200, 0.2),\n",
    "        'bottom_line': (30, 0.15)\n",
    "    }\n",
    "    \n",
    "    for field, (min_length, points) in required_fields.items():\n",
    "        content = intelligence_data.get(field, '')\n",
    "        if len(content) >= min_length:\n",
    "            score += points\n",
    "        elif len(content) >= min_length * 0.5:  # Partial credit\n",
    "            score += points * 0.5\n",
    "    \n",
    "    # Optional but valuable fields\n",
    "    optional_fields = [\n",
    "        'strategic_implications', 'technical_developments', 'market_dynamics',\n",
    "        'key_people', 'companies_mentioned', 'actionable_insights'\n",
    "    ]\n",
    "    \n",
    "    populated_optional = 0\n",
    "    for field in optional_fields:\n",
    "        data = intelligence_data.get(field, [])\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            # Check if list items have substance\n",
    "            substantial_items = [item for item in data if len(str(item)) > 20]\n",
    "            if substantial_items:\n",
    "                populated_optional += 1\n",
    "    \n",
    "    # Bonus for populated optional fields (0-0.3 points)\n",
    "    optional_score = min(populated_optional / len(optional_fields), 1.0) * 0.3\n",
    "    score += optional_score\n",
    "    \n",
    "    # Quality indicators\n",
    "    summary = intelligence_data.get('executive_summary', '')\n",
    "    if len(summary.split('.')) >= 4:  # Multiple sentences\n",
    "        score += 0.1\n",
    "    if any(word in summary.lower() for word in ['strategic', 'impact', 'business', 'market']):\n",
    "        score += 0.1\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "def _assess_analysis_depth(self, intelligence_data: Dict) -> float:\n",
    "    \"\"\"Assess depth and quality of analysis (0-1)\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Check for specific, actionable insights\n",
    "    insights = intelligence_data.get('actionable_insights', [])\n",
    "    if isinstance(insights, list):\n",
    "        # Quality of insights (look for specific language)\n",
    "        quality_indicators = ['implement', 'evaluate', 'consider', 'monitor', 'assess', 'develop']\n",
    "        substantial_insights = 0\n",
    "        \n",
    "        for insight in insights:\n",
    "            if len(str(insight)) > 30:  # Substantial length\n",
    "                if any(indicator in str(insight).lower() for indicator in quality_indicators):\n",
    "                    substantial_insights += 1\n",
    "        \n",
    "        score += min(substantial_insights * 0.15, 0.45)  # Up to 0.45 points\n",
    "    \n",
    "    # Check for quantified impacts\n",
    "    quantified = intelligence_data.get('quantified_impact', [])\n",
    "    if isinstance(quantified, list) and len(quantified) > 0:\n",
    "        # Look for actual numbers\n",
    "        has_numbers = any(any(char.isdigit() for char in str(item)) for item in quantified)\n",
    "        if has_numbers:\n",
    "            score += 0.2\n",
    "        else:\n",
    "            score += 0.1\n",
    "    \n",
    "    # Check for specific company/people mentions with context\n",
    "    companies = intelligence_data.get('companies_mentioned', [])\n",
    "    people = intelligence_data.get('key_people', [])\n",
    "    \n",
    "    # Quality check - look for context, not just names\n",
    "    quality_companies = [c for c in companies if ' - ' in str(c) and len(str(c)) > 30]\n",
    "    quality_people = [p for p in people if ' - ' in str(p) or '(' in str(p)]\n",
    "    \n",
    "    if quality_companies or quality_people:\n",
    "        score += 0.15\n",
    "    \n",
    "    # Check importance score reasonableness\n",
    "    importance = intelligence_data.get('importance_score', 5)\n",
    "    if 3 <= importance <= 9:  # Reasonable range\n",
    "        score += 0.1\n",
    "    \n",
    "    # Bonus for predictions with timelines or specificity\n",
    "    predictions = intelligence_data.get('predictions', [])\n",
    "    if isinstance(predictions, list) and predictions:\n",
    "        specific_predictions = [p for p in predictions if any(term in str(p).lower() for term in ['2024', '2025', 'months', 'years', 'by'])]\n",
    "        if specific_predictions:\n",
    "            score += 0.1\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "def _assess_processing_quality(self, processing_time: float, intelligence_data: Dict) -> float:\n",
    "    \"\"\"Assess processing quality indicators (0-1)\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Processing time indicators\n",
    "    if 10 <= processing_time <= 60:  # Sweet spot - not too fast (shallow) or slow (struggles)\n",
    "        score += 0.4\n",
    "    elif 5 <= processing_time <= 90:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Check for parsing success (no fallback indicators)\n",
    "    if not intelligence_data.get('parsing_error'):\n",
    "        score += 0.3\n",
    "    \n",
    "    if not intelligence_data.get('fallback_created'):\n",
    "        score += 0.2\n",
    "    \n",
    "    # Cost reasonableness (indicates proper token usage)\n",
    "    cost = intelligence_data.get('processing_cost', 0)\n",
    "    if 0.01 <= cost <= 0.10:  # Reasonable cost range\n",
    "        score += 0.1\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: Apply the new methods to your system object\n",
    "# ==============================================================================\n",
    "\n",
    "# Add these lines at the very end of Cell 3.5, after the existing method assignments:\n",
    "system.calculate_confidence_score = calculate_confidence_score.__get__(system, ProductionPodcastIntelligence)\n",
    "system._assess_source_quality = _assess_source_quality.__get__(system, ProductionPodcastIntelligence)\n",
    "system._assess_content_completeness = _assess_content_completeness.__get__(system, ProductionPodcastIntelligence)\n",
    "system._assess_analysis_depth = _assess_analysis_depth.__get__(system, ProductionPodcastIntelligence)\n",
    "system._assess_processing_quality = _assess_processing_quality.__get__(system, ProductionPodcastIntelligence)\n",
    "\n",
    "print(\"‚úÖ Confidence scoring system added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fe72cf-1a16-45ce-bce4-6fbf06f9415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ PROCESSING PODCASTS\n",
      "======================================================================\n",
      "2025-09-23 19:43:32,470 - PodcastIntelligence - INFO - === Starting enhanced podcast processing batch ===\n",
      "2025-09-23 19:43:32,471 - PodcastIntelligence - INFO - Processing 8 active podcasts\n",
      "2025-09-23 19:43:32,472 - PodcastIntelligence - INFO - Estimated cost: $0.4000\n",
      "2025-09-23 19:43:32,474 - PodcastIntelligence - INFO - Processing Practical AI (priority: high)\n",
      "2025-09-23 19:43:32,475 - PodcastIntelligence - INFO - Fetching episodes from Practical AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/mj5hq11s3hd09fwmr3_7mqpw0000gn/T/ipykernel_2621/1079899100.py:566: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('''\n",
      "/var/folders/wy/mj5hq11s3hd09fwmr3_7mqpw0000gn/T/ipykernel_2621/1079899100.py:574: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 19:43:35,521 - PodcastIntelligence - INFO - Found 0 new episodes from Practical AI\n",
      "2025-09-23 19:43:35,523 - PodcastIntelligence - INFO - No new episodes found for Practical AI\n",
      "2025-09-23 19:43:35,524 - PodcastIntelligence - INFO - Processing Last Week in AI (priority: high)\n",
      "2025-09-23 19:43:35,525 - PodcastIntelligence - INFO - Fetching episodes from Last Week in AI\n",
      "2025-09-23 19:43:36,419 - PodcastIntelligence - INFO - Found 1 new episodes from Last Week in AI\n",
      "2025-09-23 19:43:58,295 - PodcastIntelligence - INFO - Successfully extracted enhanced intelligence from Last Week in AI #322 - Robotaxi progress, OpenAI Business, Gemini in Chrome ($0.0204, 21.9s)\n",
      "2025-09-23 19:43:58,301 - PodcastIntelligence - INFO - ‚úÖ Processed: Last Week in AI #322 - Robotaxi progress, OpenAI B...\n",
      "2025-09-23 19:43:58,302 - PodcastIntelligence - INFO - ‚úÖ Last Week in AI: 1 episodes processed in 22.8s\n",
      "2025-09-23 19:43:58,303 - PodcastIntelligence - INFO - Processing The AI Podcast (priority: medium)\n",
      "2025-09-23 19:43:58,304 - PodcastIntelligence - INFO - Fetching episodes from The AI Podcast\n",
      "2025-09-23 19:43:59,201 - PodcastIntelligence - INFO - Found 1 new episodes from The AI Podcast\n",
      "2025-09-23 19:43:59,204 - PodcastIntelligence - INFO - ‚ö†Ô∏è The AI Podcast: No episodes processed\n",
      "2025-09-23 19:43:59,205 - PodcastIntelligence - INFO - Processing Lex Fridman Podcast (priority: high)\n",
      "2025-09-23 19:43:59,205 - PodcastIntelligence - INFO - Fetching episodes from Lex Fridman Podcast\n",
      "2025-09-23 19:44:02,756 - PodcastIntelligence - INFO - Found 1 new episodes from Lex Fridman Podcast\n",
      "2025-09-23 19:44:31,482 - PodcastIntelligence - INFO - Successfully extracted enhanced intelligence from #481 ‚Äì Norman Ohler: Hitler, Nazis, Drugs, WW2, Blitzkrieg, LSD, MKUltra & CIA ($0.0297, 28.7s)\n",
      "2025-09-23 19:44:31,488 - PodcastIntelligence - INFO - ‚úÖ Processed: #481 ‚Äì Norman Ohler: Hitler, Nazis, Drugs, WW2, Bl...\n",
      "2025-09-23 19:44:31,489 - PodcastIntelligence - INFO - ‚úÖ Lex Fridman Podcast: 1 episodes processed in 32.3s\n",
      "2025-09-23 19:44:31,490 - PodcastIntelligence - INFO - Processing Eye on AI (priority: high)\n",
      "2025-09-23 19:44:31,491 - PodcastIntelligence - INFO - Fetching episodes from Eye on AI\n",
      "2025-09-23 19:44:32,204 - PodcastIntelligence - WARNING - No episodes found for Eye on AI\n",
      "2025-09-23 19:44:32,205 - PodcastIntelligence - INFO - No new episodes found for Eye on AI\n",
      "2025-09-23 19:44:32,206 - PodcastIntelligence - INFO - Processing AI Today Podcast (priority: medium)\n",
      "2025-09-23 19:44:32,206 - PodcastIntelligence - INFO - Fetching episodes from AI Today Podcast\n",
      "2025-09-23 19:44:32,399 - PodcastIntelligence - WARNING - No episodes found for AI Today Podcast\n",
      "2025-09-23 19:44:32,400 - PodcastIntelligence - INFO - No new episodes found for AI Today Podcast\n",
      "2025-09-23 19:44:32,400 - PodcastIntelligence - INFO - Processing The AI Show (priority: medium)\n",
      "2025-09-23 19:44:32,401 - PodcastIntelligence - INFO - Fetching episodes from The AI Show\n",
      "2025-09-23 19:44:33,839 - PodcastIntelligence - WARNING - No episodes found for The AI Show\n",
      "2025-09-23 19:44:33,840 - PodcastIntelligence - INFO - No new episodes found for The AI Show\n",
      "2025-09-23 19:44:33,841 - PodcastIntelligence - INFO - Processing MIT AI Podcast (priority: medium)\n",
      "2025-09-23 19:44:33,842 - PodcastIntelligence - INFO - Fetching episodes from MIT AI Podcast\n",
      "2025-09-23 19:44:34,180 - PodcastIntelligence - INFO - Found 1 new episodes from MIT AI Podcast\n",
      "2025-09-23 19:45:04,056 - PodcastIntelligence - INFO - Successfully extracted enhanced intelligence from The beginner's guide to coding with Cursor | Lee Robinson (Head of AI education) ($0.0337, 29.9s)\n",
      "2025-09-23 19:45:04,063 - PodcastIntelligence - INFO - ‚úÖ Processed: The beginner's guide to coding with Cursor | Lee R...\n",
      "2025-09-23 19:45:04,064 - PodcastIntelligence - INFO - ‚úÖ MIT AI Podcast: 1 episodes processed in 30.2s\n",
      "2025-09-23 19:45:04,064 - PodcastIntelligence - INFO - === Enhanced podcast processing batch complete ===\n",
      "2025-09-23 19:45:04,065 - PodcastIntelligence - INFO - Total time: 91.6s\n",
      "2025-09-23 19:45:04,066 - PodcastIntelligence - INFO - Episodes processed: 3\n",
      "2025-09-23 19:45:04,067 - PodcastIntelligence - INFO - Episodes failed: 0\n",
      "2025-09-23 19:45:04,068 - PodcastIntelligence - INFO - Episodes skipped: 1\n",
      "2025-09-23 19:45:04,069 - PodcastIntelligence - INFO - Total cost: $0.0837\n",
      "2025-09-23 19:45:04,069 - PodcastIntelligence - INFO - Podcasts successful: 3\n",
      "\n",
      "üìä PROCESSING RESULTS:\n",
      "‚úÖ Episodes processed: 3\n",
      "‚ùå Episodes failed: 0\n",
      "‚è≠Ô∏è Episodes skipped: 1\n",
      "üí∞ Total cost: $0.0837\n",
      "‚è±Ô∏è Processing time: 91.6 seconds\n",
      "üìª Podcasts processed: 3\n",
      "\n",
      "üìã TOP EPISODES:\n",
      "üìª Last Week in AI - Last Week in AI #322 - Robotaxi progress, OpenAI B...\n",
      "   ‚≠ê Importance: 8/10 | üí∞ $0.0204\n",
      "üìª Lex Fridman Podcast - #481 ‚Äì Norman Ohler: Hitler, Nazis, Drugs, WW2, Bl...\n",
      "   ‚≠ê Importance: 7/10 | üí∞ $0.0297\n",
      "üìª MIT AI Podcast - The beginner's guide to coding with Cursor | Lee R...\n",
      "   ‚≠ê Importance: 8/10 | üí∞ $0.0337\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Process Podcasts\n",
    "if system:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ PROCESSING PODCASTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process all configured podcasts\n",
    "    results = system.process_all_podcasts()\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"‚ùå Processing failed: {results['error']}\")\n",
    "    else:\n",
    "        print(f\"\\nüìä PROCESSING RESULTS:\")\n",
    "        print(f\"‚úÖ Episodes processed: {results['processed_episodes']}\")\n",
    "        print(f\"‚ùå Episodes failed: {results['failed_episodes']}\")\n",
    "        print(f\"‚è≠Ô∏è Episodes skipped: {results['skipped_episodes']}\")\n",
    "        print(f\"üí∞ Total cost: ${results['total_cost']:.4f}\")\n",
    "        print(f\"‚è±Ô∏è Processing time: {results['batch_processing_time']:.1f} seconds\")\n",
    "        print(f\"üìª Podcasts processed: {len(results['podcasts_processed'])}\")\n",
    "        \n",
    "        if results['episode_details']:\n",
    "            print(f\"\\nüìã TOP EPISODES:\")\n",
    "            for episode in results['episode_details'][:3]:\n",
    "                print(f\"üìª {episode['podcast']} - {episode['title'][:50]}...\")\n",
    "                print(f\"   ‚≠ê Importance: {episode['importance']}/10 | üí∞ ${episode['cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68587a47-641b-4eb1-8d72-8fa53a3c20ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÑ GENERATING INTELLIGENCE REPORTS\n",
      "======================================================================\n",
      "2025-09-23 19:45:04,076 - PodcastIntelligence - INFO - Generating enhanced intelligence reports...\n",
      "2025-09-23 19:45:04,081 - PodcastIntelligence - INFO - Reports saved: AI_Intelligence_Report_20250923_194504.html\n",
      "‚úÖ Reports generated!\n",
      "üìÑ HTML: reports/AI_Intelligence_Report_20250923_194504.html\n",
      "üìù Markdown: reports/AI_Intelligence_Report_20250923_194504.md\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <!DOCTYPE html>\n",
       "        <html lang=\"en\">\n",
       "        <head>\n",
       "            <meta charset=\"UTF-8\">\n",
       "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "            <title>AI Intelligence Executive Brief - September 23, 2025 at 07:45 PM</title>\n",
       "            <style>\n",
       "                body {\n",
       "                    font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, sans-serif;\n",
       "                    line-height: 1.6; color: #2c3e50; max-width: 1200px; margin: 0 auto;\n",
       "                    padding: 20px; background-color: #f8f9fa;\n",
       "                }\n",
       "                .header {\n",
       "                    background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);\n",
       "                    color: white; padding: 40px 30px; border-radius: 15px; text-align: center;\n",
       "                    margin-bottom: 30px; box-shadow: 0 10px 30px rgba(0,0,0,0.15);\n",
       "                }\n",
       "                .executive-summary {\n",
       "                    background: white; border-left: 6px solid #e74c3c; padding: 30px;\n",
       "                    margin: 30px 0; border-radius: 10px; box-shadow: 0 6px 20px rgba(0,0,0,0.1);\n",
       "                }\n",
       "                .stats-grid {\n",
       "                    display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));\n",
       "                    gap: 20px; margin: 30px 0;\n",
       "                }\n",
       "                .stat-card {\n",
       "                    background: white; padding: 25px; border-radius: 12px; text-align: center;\n",
       "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1); border-top: 4px solid #3498db;\n",
       "                }\n",
       "                .section {\n",
       "                    background: white; margin: 30px 0; padding: 35px; border-radius: 12px;\n",
       "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
       "                }\n",
       "                .episode-card {\n",
       "                    background: #f8f9fa; margin: 25px 0; padding: 30px; border-radius: 12px;\n",
       "                    border-left: 6px solid #3498db;\n",
       "                }\n",
       "                .tag {\n",
       "                    background: linear-gradient(135deg, #3498db, #2980b9); color: white;\n",
       "                    padding: 8px 14px; border-radius: 20px; font-size: 0.9em; margin: 3px;\n",
       "                    display: inline-block;\n",
       "                }\n",
       "                .episode-link { margin-top: 15px; padding-top: 15px; border-top: 1px solid #ecf0f1; }\n",
       "                .episode-link a { color: #3498db; text-decoration: none; font-weight: 500; }\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"header\">\n",
       "                <h1>AI Intelligence Executive Brief</h1>\n",
       "                <p>Week of September 16 - September 23, 2025</p>\n",
       "                <p>Generated on September 23, 2025 at 07:45 PM</p>\n",
       "            </div>\n",
       "\n",
       "            <div class=\"executive-summary\">\n",
       "                <h2>Executive Summary</h2>\n",
       "                <p><strong>Bottom line:</strong> This week's AI intelligence reveals 5 high-impact developments across 5 key podcasts.</p>\n",
       "                <h3>This Week's Headlines</h3>\n",
       "                <ul>\n",
       "        <li><strong>Cursor's AI-powered code editor is democratizing software development by enabling beginners to build production-quality applications through automated error fixing, parallel task execution, and intelligent code generation with proper guardrails.</strong></li>\n",
       "<li><strong>Amazon's Zoox robotaxi launch in Las Vegas and OpenAI's Microsoft-approved transition to for-profit structure signal accelerating commercialization of AI and autonomous vehicle technologies.</strong></li>\n",
       "<li><strong>A product manager with zero mobile development experience successfully built a cross-platform AI-powered fitness app using AI coding tools, demonstrating how AI can democratize complex software development for non-technical professionals.</strong></li>\n",
       "<li><strong>Historical analysis reveals how psychoactive substances fundamentally shaped military strategy, decision-making, and human civilization, offering critical insights for understanding performance enhancement, risk assessment, and organizational behavior in modern contexts.</strong></li>\n",
       "\n",
       "                </ul>\n",
       "            </div>\n",
       "\n",
       "            <div class=\"stats-grid\">\n",
       "                <div class=\"stat-card\"><h3>6</h3><p>Episodes</p></div>\n",
       "                <div class=\"stat-card\"><h3>5</h3><p>High-Impact</p></div>\n",
       "                <div class=\"stat-card\"><h3>$0.162</h3><p>Cost</p></div>\n",
       "                <div class=\"stat-card\"><h3>7.8/10</h3><p>Avg Importance</p></div>\n",
       "            </div>\n",
       "\n",
       "            <div class=\"section\">\n",
       "                <h2>Companies Mentioned</h2>\n",
       "        <span class=\"tag\">Cursor - AI-powered code editor featured as the main subject, positioning itself as a bridge between beginner and expert developers (1)</span>\n",
       "<span class=\"tag\">Vercel - Previous company where Lee Robinson helped build the platform as an early employee (1)</span>\n",
       "<span class=\"tag\">Next.js - Framework that Lee Robinson helped develop, providing credibility for his current AI education role (1)</span>\n",
       "<span class=\"tag\">Google Gemini - Sponsor mentioned as an everyday AI assistant (1)</span>\n",
       "<span class=\"tag\">Persona - Sponsor providing identity verification services (1)</span>\n",
       "<span class=\"tag\">Amazon - Launching Zoox robotaxi service in Las Vegas, entering autonomous vehicle market (1)</span>\n",
       "<span class=\"tag\">Zoox - Amazon's autonomous vehicle subsidiary beginning commercial operations (1)</span>\n",
       "<span class=\"tag\">OpenAI - Securing Microsoft approval for for-profit transition and business expansion (1)</span>\n",
       "\n",
       "            </div>\n",
       "\n",
       "            <div class=\"section\">\n",
       "                <h2>Episode Intelligence</h2>\n",
       "        \n",
       "            <div class=\"episode-card\">\n",
       "                <h3>1. MIT AI Podcast</h3>\n",
       "                <h4>The beginner's guide to coding with Cursor | Lee Robinson (Head of AI education)</h4>\n",
       "                <p><strong>Headline:</strong> Cursor's AI-powered code editor is democratizing software development by enabling beginners to build production-quality applications through automated error fixing, parallel task execution, and intelligent code generation with proper guardrails.</p>\n",
       "                <p>Cursor's AI-powered code editor is democratizing software development by enabling beginners to build production-quality applications through automated error fixing, parallel task execution, and intelligent code generation with proper guardrails. Lee Robinson, Head of AI Education at Cursor and former Vercel/Next.js early employee, demonstrates how the platform bridges the skill gap between novice and experienced developers by automating complex tasks like linting, debugging, and code reviews. The key innovation lies in establishing proper development guardrails‚Äîtyped languages, linters, formatters, and tests‚Äîthat help AI tools generate better, more reliable code while reducing technical debt. Organizations can leverage this technology to accelerate development cycles by running parallel coding tasks, where developers focus on core features while AI agents handle secondary implementations in the background. The platform also extends beyond coding to improve business writing by eliminating AI-generated patterns and clich√©s through custom prompts and banned word lists. This represents a fundamental shift in how companies can approach software development talent acquisition and productivity, potentially reducing the barrier to entry for technical roles while enhancing output quality across development teams.</p>\n",
       "                <p><strong>Impact:</strong> 8/10 | <strong>Cost:</strong> $0.0337</p>\n",
       "                <div class=\"episode-link\"><a href=\"https://podcasters.spotify.com/pod/show/pen-name/episodes/The-beginners-guide-to-coding-with-Cursor--Lee-Robinson-Head-of-AI-education-e38b04g\" target=\"_blank\">Listen to Episode</a></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"episode-card\">\n",
       "                <h3>2. Last Week in AI</h3>\n",
       "                <h4>Last Week in AI #322 - Robotaxi progress, OpenAI Business, Gemini in Chrome</h4>\n",
       "                <p><strong>Headline:</strong> Amazon's Zoox robotaxi launch in Las Vegas and OpenAI's Microsoft-approved transition to for-profit structure signal accelerating commercialization of AI and autonomous vehicle technologies.</p>\n",
       "                <p>Amazon's Zoox robotaxi launch in Las Vegas and OpenAI's Microsoft-approved transition to for-profit structure signal accelerating commercialization of AI and autonomous vehicle technologies. Amazon is making its first major move into the competitive U.S. robotaxi market, directly challenging established players like Waymo and Cruise while leveraging its logistics expertise and AWS infrastructure. OpenAI's successful negotiation with Microsoft to restructure as a for-profit entity removes a significant governance constraint and positions the company for more aggressive expansion and investment opportunities. These developments reflect a broader industry shift toward practical deployment of AI technologies in consumer-facing applications. The convergence of autonomous vehicles and AI business model evolution suggests we're entering a new phase of technology commercialization with significant implications for transportation, logistics, and enterprise AI adoption.</p>\n",
       "                <p><strong>Impact:</strong> 8/10 | <strong>Cost:</strong> $0.0204</p>\n",
       "                <div class=\"episode-link\"><a href=\"https://lastweekin.ai/p/last-week-in-ai-322-robotaxi-progress\" target=\"_blank\">Listen to Episode</a></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"episode-card\">\n",
       "                <h3>3. AI Today Podcast</h3>\n",
       "                <h4>How I built an Apple Watch workout app using Cursor and Xcode (with zero mobile-app experience)</h4>\n",
       "                <p><strong>Headline:</strong> A product manager with zero mobile development experience successfully built a cross-platform AI-powered fitness app using AI coding tools, demonstrating how AI can democratize complex software development for non-technical professionals.</p>\n",
       "                <p>Terry Lin, a product manager, created Cooper's Corner, a voice-powered fitness tracking app for iPhone and Apple Watch, despite having no prior mobile development experience, using AI coding tools like Cursor and traditional development environments like Xcode. His success demonstrates a replicable methodology combining AI-assisted coding with structured workflows that could enable product managers and business professionals to build sophisticated applications independently. The case study reveals specific techniques for optimizing codebases for AI collaboration, including file size management, structured prompting, and iterative refinement processes. Lin's approach suggests that AI coding tools are mature enough to enable non-developers to create production-quality mobile applications, potentially disrupting traditional software development team structures. His three-step process (create, review, execute) and dual-tool workflow provide a blueprint for organizations looking to leverage AI for rapid prototyping and development. The episode highlights how AI tools can bridge the gap between business vision and technical execution, enabling faster time-to-market for digital products.</p>\n",
       "                <p><strong>Impact:</strong> 8/10 | <strong>Cost:</strong> $0.0280</p>\n",
       "                <div class=\"episode-link\"><a href=\"https://podcasters.spotify.com/pod/show/pen-name/episodes/How-I-built-an-Apple-Watch-workout-app-using-Cursor-and-Xcode-with-zero-mobile-app-experience-e380v7j\" target=\"_blank\">Listen to Episode</a></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"episode-card\">\n",
       "                <h3>4. The AI Podcast</h3>\n",
       "                <h4>Bringing Robots to Life with AI: The Three Computer Revolution - Ep. 274</h4>\n",
       "                <p><strong>Headline:</strong> None</p>\n",
       "                <p>NVIDIA's three-computer robotics solution (DGX training, Omniverse simulation, Jetson inference) is creating a standardized full-stack approach that could accelerate robotics deployment across industries by solving the critical sim-to-real transfer problem. | NVIDIA's three-computer robotics solution represents a potential industry standardization that could dramatically accelerate robotics adoption across sectors. The integrated approach combines DGX systems for AI model training, Omniverse and Cosmos platforms for realistic simulation environments, and Jetson AGX hardware for real-time robot inference, creating a seamless pipeline from development to deployment. This full-stack solution directly addresses the industry's biggest challenge: the gap between simulated training environments and real-world performance that has historically limited robotics scalability. NVIDIA's Seattle Robotics Lab is positioning this as a comprehensive platform play, similar to how their GPU ecosystem dominated AI training, potentially creating significant competitive advantages for early adopters. The focus on humanoid robotics and adaptive intelligence suggests NVIDIA sees this as a foundational technology for the next wave of automation across manufacturing, logistics, and service industries.</p>\n",
       "                <p><strong>Impact:</strong> 8/10 | <strong>Cost:</strong> $0.0253</p>\n",
       "                \n",
       "            </div>\n",
       "            \n",
       "            <div class=\"episode-card\">\n",
       "                <h3>5. The AI Podcast</h3>\n",
       "                <h4>Bringing Robots to Life with AI: The Three Computer Revolution - Ep. 274</h4>\n",
       "                <p><strong>Headline:</strong> None</p>\n",
       "                <p>NVIDIA's three-computer robotics architecture‚Äîcombining cloud training, simulation environments, and edge inference‚Äîis creating a standardized full-stack solution that could accelerate enterprise robotics adoption across industries. | NVIDIA's three-computer robotics architecture represents a comprehensive approach to solving the deployment challenges that have historically limited enterprise robotics adoption. Yashraj Narang, head of NVIDIA's Seattle Robotics Lab, outlined how DGX systems handle AI model training, Omniverse and Cosmos platforms enable realistic simulation and testing, while Jetson AGX devices provide real-time inference at the edge. This integrated approach addresses the critical sim-to-real gap that has prevented robots from adapting effectively to unpredictable real-world environments. The strategy positions NVIDIA to capture value across the entire robotics development lifecycle, from initial training through deployment, while potentially standardizing how enterprises approach robotics integration. For executives, this signals both an opportunity to leverage more capable robotics solutions and a need to evaluate how NVIDIA's platform approach might impact competitive dynamics in automation investments.</p>\n",
       "                <p><strong>Impact:</strong> 8/10 | <strong>Cost:</strong> $0.0254</p>\n",
       "                \n",
       "            </div>\n",
       "            \n",
       "            <div class=\"episode-card\">\n",
       "                <h3>6. Lex Fridman Podcast</h3>\n",
       "                <h4>#481 ‚Äì Norman Ohler: Hitler, Nazis, Drugs, WW2, Blitzkrieg, LSD, MKUltra & CIA</h4>\n",
       "                <p><strong>Headline:</strong> Historical analysis reveals how psychoactive substances fundamentally shaped military strategy, decision-making, and human civilization, offering critical insights for understanding performance enhancement, risk assessment, and organizational behavior in modern contexts.</p>\n",
       "                <p>Historian Norman Ohler's research demonstrates that psychoactive substances fundamentally shaped military strategy, decision-making, and human civilization, providing critical frameworks for understanding performance enhancement and organizational behavior today. His investigation into Nazi Germany's systematic use of methamphetamine (Pervitin) during WWII reveals how chemical enhancement enabled the Blitzkrieg strategy while ultimately contributing to strategic failures through impaired judgment and addiction. The discussion extends to CIA's MKUltra program and broader patterns of substance use throughout human history, illustrating how chemical influences on cognition affect leadership, innovation, and societal development. Ohler's work challenges conventional historical narratives by incorporating previously overlooked pharmaceutical evidence, demonstrating the importance of interdisciplinary analysis in understanding complex systems. The conversation provides valuable insights into risk assessment, performance optimization, and the unintended consequences of enhancement technologies. These historical patterns offer strategic frameworks for evaluating modern performance enhancement tools, understanding decision-making under chemical influence, and assessing long-term organizational risks.</p>\n",
       "                <p><strong>Impact:</strong> 7/10 | <strong>Cost:</strong> $0.0297</p>\n",
       "                <div class=\"episode-link\"><a href=\"https://lexfridman.com/norman-ohler/?utm_source=rss&utm_medium=rss&utm_campaign=norman-ohler\" target=\"_blank\">Listen to Episode</a></div>\n",
       "            </div>\n",
       "            \n",
       "            </div>\n",
       "        </body>\n",
       "        </html>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ AI Podcast Intelligence System Complete!\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Generate Reports\n",
    "if system and 'results' in locals() and results.get('processed_episodes', 0) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÑ GENERATING INTELLIGENCE REPORTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate reports\n",
    "    html_report, markdown_report = system.generate_enhanced_report()\n",
    "    \n",
    "    # Save reports\n",
    "    html_file, md_file = system.save_and_display_reports(html_report, markdown_report)\n",
    "    \n",
    "    print(f\"‚úÖ Reports generated!\")\n",
    "    print(f\"üìÑ HTML: {html_file}\")\n",
    "    print(f\"üìù Markdown: {md_file}\")\n",
    "    \n",
    "    # Display report\n",
    "    from IPython.display import HTML, display\n",
    "    display(HTML(html_report))\n",
    "    \n",
    "print(\"\\nüéâ AI Podcast Intelligence System Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bdcce4e-ba14-4bc7-9580-3ef78ad05eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ PROCESSING WITH ENHANCED CONFIDENCE SCORING\n",
      "======================================================================\n",
      "2025-09-23 19:45:04,093 - PodcastIntelligence - INFO - === Starting enhanced podcast processing batch ===\n",
      "2025-09-23 19:45:04,094 - PodcastIntelligence - INFO - Processing 8 active podcasts\n",
      "2025-09-23 19:45:04,095 - PodcastIntelligence - INFO - Estimated cost: $0.4000\n",
      "2025-09-23 19:45:04,101 - PodcastIntelligence - INFO - Processing Practical AI (priority: high)\n",
      "2025-09-23 19:45:04,102 - PodcastIntelligence - INFO - Fetching episodes from Practical AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/mj5hq11s3hd09fwmr3_7mqpw0000gn/T/ipykernel_2621/1079899100.py:566: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('''\n",
      "/var/folders/wy/mj5hq11s3hd09fwmr3_7mqpw0000gn/T/ipykernel_2621/1079899100.py:574: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-23 19:45:07,309 - PodcastIntelligence - INFO - Found 0 new episodes from Practical AI\n",
      "2025-09-23 19:45:07,311 - PodcastIntelligence - INFO - No new episodes found for Practical AI\n",
      "2025-09-23 19:45:07,311 - PodcastIntelligence - INFO - Processing Last Week in AI (priority: high)\n",
      "2025-09-23 19:45:07,312 - PodcastIntelligence - INFO - Fetching episodes from Last Week in AI\n",
      "2025-09-23 19:45:07,631 - PodcastIntelligence - INFO - Found 1 new episodes from Last Week in AI\n",
      "2025-09-23 19:45:07,635 - PodcastIntelligence - INFO - ‚ö†Ô∏è Last Week in AI: No episodes processed\n",
      "2025-09-23 19:45:07,635 - PodcastIntelligence - INFO - Processing The AI Podcast (priority: medium)\n",
      "2025-09-23 19:45:07,636 - PodcastIntelligence - INFO - Fetching episodes from The AI Podcast\n",
      "2025-09-23 19:45:08,607 - PodcastIntelligence - INFO - Found 1 new episodes from The AI Podcast\n",
      "2025-09-23 19:45:08,610 - PodcastIntelligence - INFO - ‚ö†Ô∏è The AI Podcast: No episodes processed\n",
      "2025-09-23 19:45:08,611 - PodcastIntelligence - INFO - Processing Lex Fridman Podcast (priority: high)\n",
      "2025-09-23 19:45:08,611 - PodcastIntelligence - INFO - Fetching episodes from Lex Fridman Podcast\n",
      "2025-09-23 19:45:12,120 - PodcastIntelligence - INFO - Found 1 new episodes from Lex Fridman Podcast\n",
      "2025-09-23 19:45:12,124 - PodcastIntelligence - INFO - ‚ö†Ô∏è Lex Fridman Podcast: No episodes processed\n",
      "2025-09-23 19:45:12,125 - PodcastIntelligence - INFO - Processing Eye on AI (priority: high)\n",
      "2025-09-23 19:45:12,125 - PodcastIntelligence - INFO - Fetching episodes from Eye on AI\n",
      "2025-09-23 19:45:12,458 - PodcastIntelligence - WARNING - No episodes found for Eye on AI\n",
      "2025-09-23 19:45:12,459 - PodcastIntelligence - INFO - No new episodes found for Eye on AI\n",
      "2025-09-23 19:45:12,459 - PodcastIntelligence - INFO - Processing AI Today Podcast (priority: medium)\n",
      "2025-09-23 19:45:12,460 - PodcastIntelligence - INFO - Fetching episodes from AI Today Podcast\n",
      "2025-09-23 19:45:12,598 - PodcastIntelligence - WARNING - No episodes found for AI Today Podcast\n",
      "2025-09-23 19:45:12,599 - PodcastIntelligence - INFO - No new episodes found for AI Today Podcast\n",
      "2025-09-23 19:45:12,600 - PodcastIntelligence - INFO - Processing The AI Show (priority: medium)\n",
      "2025-09-23 19:45:12,601 - PodcastIntelligence - INFO - Fetching episodes from The AI Show\n",
      "2025-09-23 19:45:13,102 - PodcastIntelligence - WARNING - No episodes found for The AI Show\n",
      "2025-09-23 19:45:13,103 - PodcastIntelligence - INFO - No new episodes found for The AI Show\n",
      "2025-09-23 19:45:13,103 - PodcastIntelligence - INFO - Processing MIT AI Podcast (priority: medium)\n",
      "2025-09-23 19:45:13,104 - PodcastIntelligence - INFO - Fetching episodes from MIT AI Podcast\n",
      "2025-09-23 19:45:13,427 - PodcastIntelligence - INFO - Found 1 new episodes from MIT AI Podcast\n",
      "2025-09-23 19:45:13,430 - PodcastIntelligence - INFO - ‚ö†Ô∏è MIT AI Podcast: No episodes processed\n",
      "2025-09-23 19:45:13,430 - PodcastIntelligence - INFO - === Enhanced podcast processing batch complete ===\n",
      "2025-09-23 19:45:13,431 - PodcastIntelligence - INFO - Total time: 9.3s\n",
      "2025-09-23 19:45:13,432 - PodcastIntelligence - INFO - Episodes processed: 0\n",
      "2025-09-23 19:45:13,433 - PodcastIntelligence - INFO - Episodes failed: 0\n",
      "2025-09-23 19:45:13,433 - PodcastIntelligence - INFO - Episodes skipped: 4\n",
      "2025-09-23 19:45:13,434 - PodcastIntelligence - INFO - Total cost: $0.0000\n",
      "2025-09-23 19:45:13,435 - PodcastIntelligence - INFO - Podcasts successful: 0\n",
      "\n",
      "üìä ENHANCED PROCESSING RESULTS:\n",
      "‚úÖ Episodes processed: 0\n",
      "‚ùå Episodes failed: 0\n",
      "‚≠ê Episodes skipped: 4\n",
      "üí∞ Total cost: $0.0000\n",
      "‚è±Ô∏è Processing time: 9.3 seconds\n",
      "üìª Podcasts processed: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Enhanced Processing\n",
    "if system:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ PROCESSING WITH ENHANCED CONFIDENCE SCORING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process all configured podcasts\n",
    "    results = system.process_all_podcasts()\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"‚ùå Processing failed: {results['error']}\")\n",
    "    else:\n",
    "        print(f\"\\nüìä ENHANCED PROCESSING RESULTS:\")\n",
    "        print(f\"‚úÖ Episodes processed: {results['processed_episodes']}\")\n",
    "        print(f\"‚ùå Episodes failed: {results['failed_episodes']}\")\n",
    "        print(f\"‚≠ê Episodes skipped: {results['skipped_episodes']}\")\n",
    "        print(f\"üí∞ Total cost: ${results['total_cost']:.4f}\")\n",
    "        print(f\"‚è±Ô∏è Processing time: {results['batch_processing_time']:.1f} seconds\")\n",
    "        print(f\"üìª Podcasts processed: {len(results['podcasts_processed'])}\")\n",
    "        \n",
    "        # Show confidence scores\n",
    "        if results['episode_details']:\n",
    "            print(f\"\\nüìã EPISODE INTELLIGENCE WITH CONFIDENCE:\")\n",
    "            for episode in results['episode_details'][:5]:\n",
    "                confidence = episode.get('confidence', 'N/A')\n",
    "                print(f\"üìª {episode['podcast']} - {episode['title'][:50]}...\")\n",
    "                print(f\"   ‚≠ê Importance: {episode['importance']}/10 | üéØ Confidence: {confidence:.3f} | üí∞ ${episode['cost']:.4f}\")\n",
    "                print(f\"   üìà {episode['headline']}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07f4c98-c051-44e9-81aa-ff967344439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Enhanced AI Podcast Intelligence System Complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Enhanced Reporting\n",
    "if system and 'results' in locals() and results.get('processed_episodes', 0) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÑ GENERATING ENHANCED INTELLIGENCE REPORTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate reports with confidence scores\n",
    "    html_report, markdown_report = system.generate_enhanced_report()\n",
    "    \n",
    "    # Save reports\n",
    "    html_file, md_file = system.save_and_display_reports(html_report, markdown_report)\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced reports generated!\")\n",
    "    print(f\"üìÑ HTML: {html_file}\")\n",
    "    print(f\"üìù Markdown: {md_file}\")\n",
    "    \n",
    "    # Show confidence distribution\n",
    "    conn = sqlite3.connect(system.db_path)\n",
    "    confidence_stats = pd.read_sql('''\n",
    "        SELECT \n",
    "            AVG(confidence_score) as avg_confidence,\n",
    "            MIN(confidence_score) as min_confidence,\n",
    "            MAX(confidence_score) as max_confidence,\n",
    "            COUNT(*) as total_episodes\n",
    "        FROM intelligence \n",
    "        WHERE processed_at >= date('now', '-7 days')\n",
    "    ''', conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\nüéØ CONFIDENCE SCORE DISTRIBUTION:\")\n",
    "    print(f\"üìä Average: {confidence_stats['avg_confidence'].iloc[0]:.3f}\")\n",
    "    print(f\"üìä Range: {confidence_stats['min_confidence'].iloc[0]:.3f} - {confidence_stats['max_confidence'].iloc[0]:.3f}\")\n",
    "    print(f\"üìä Episodes: {confidence_stats['total_episodes'].iloc[0]}\")\n",
    "    \n",
    "    # Display the HTML report\n",
    "    from IPython.display import HTML, display\n",
    "    display(HTML(html_report))\n",
    "\n",
    "print(\"\\nüéâ Enhanced AI Podcast Intelligence System Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0dc2a13-a0be-4898-be06-d18b83b0941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FINDING DUPLICATE EPISODES...\n",
      "‚úÖ No duplicate episodes found!\n",
      "üßπ Duplicate cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Run this to clean up existing duplicates in your database\n",
    "\n",
    "def cleanup_duplicate_episodes(self):\n",
    "    \"\"\"Remove duplicate episodes from database\"\"\"\n",
    "    conn = sqlite3.connect(self.db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        print(\"üîç FINDING DUPLICATE EPISODES...\")\n",
    "        \n",
    "        # Find potential duplicates\n",
    "        cursor.execute('''\n",
    "            SELECT podcast_name, title, COUNT(*) as count, GROUP_CONCAT(id) as ids\n",
    "            FROM episodes \n",
    "            GROUP BY podcast_name, title \n",
    "            HAVING COUNT(*) > 1\n",
    "            ORDER BY podcast_name, title\n",
    "        ''')\n",
    "        \n",
    "        duplicates = cursor.fetchall()\n",
    "        \n",
    "        if not duplicates:\n",
    "            print(\"‚úÖ No duplicate episodes found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìä Found {len(duplicates)} sets of duplicate episodes:\")\n",
    "        \n",
    "        removed_count = 0\n",
    "        for podcast_name, title, count, ids_str in duplicates:\n",
    "            episode_ids = [int(id_str) for id_str in ids_str.split(',')]\n",
    "            print(f\"\\nüìª {podcast_name}\")\n",
    "            print(f\"   üì∫ {title} ({count} duplicates)\")\n",
    "            \n",
    "            # Keep the first episode, remove the rest\n",
    "            keep_id = episode_ids[0]\n",
    "            remove_ids = episode_ids[1:]\n",
    "            \n",
    "            print(f\"   ‚úÖ Keeping episode ID: {keep_id}\")\n",
    "            print(f\"   üóëÔ∏è Removing episode IDs: {remove_ids}\")\n",
    "            \n",
    "            for remove_id in remove_ids:\n",
    "                # Remove intelligence data first (foreign key constraint)\n",
    "                cursor.execute('DELETE FROM intelligence WHERE episode_id = ?', (remove_id,))\n",
    "                # Remove episode\n",
    "                cursor.execute('DELETE FROM episodes WHERE id = ?', (remove_id,))\n",
    "                removed_count += 1\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"\\n‚úÖ Cleanup complete! Removed {removed_count} duplicate episodes.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during cleanup: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Run the cleanup\n",
    "system.cleanup_duplicate_episodes = cleanup_duplicate_episodes.__get__(system, ProductionPodcastIntelligence)\n",
    "system.cleanup_duplicate_episodes()\n",
    "\n",
    "print(\"üßπ Duplicate cleanup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a28bd87-7ad2-4e53-b0e6-53339b0e246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç IDENTIFYING DUPLICATE EPISODES...\n",
      "üìª The AI Podcast episodes:\n",
      "   ID 3: From AlphaFold to MMseqs2-GPU: How AI is Accelerating Protein Science\n",
      "      üìÖ Pub Date: 2025-09-10T07:00:00\n",
      "      üîó GUID: None\n",
      "      üß† Processed: 2025-09-12 18:50:53\n",
      "\n",
      "   ID 8: Bringing Robots to Life with AI: The Three Computer Revolution - Ep. 274\n",
      "      üìÖ Pub Date: 2025-09-17T15:54:00\n",
      "      üîó GUID: a0dac7d4-932b-11f0-b609-b3a5538e89a1\n",
      "      üß† Processed: 2025-09-17 18:25:19\n",
      "\n",
      "   ID 8: Bringing Robots to Life with AI: The Three Computer Revolution - Ep. 274\n",
      "      üìÖ Pub Date: 2025-09-17T15:54:00\n",
      "      üîó GUID: a0dac7d4-932b-11f0-b609-b3a5538e89a1\n",
      "      üß† Processed: 2025-09-17 18:32:59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IDENTIFY DUPLICATE EPISODES\n",
    "print(\"üîç IDENTIFYING DUPLICATE EPISODES...\")\n",
    "\n",
    "conn = sqlite3.connect(system.db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check The AI Podcast episodes specifically\n",
    "cursor.execute('''\n",
    "    SELECT e.id, e.title, e.pub_date, e.guid, i.processed_at\n",
    "    FROM episodes e\n",
    "    LEFT JOIN intelligence i ON e.id = i.episode_id\n",
    "    WHERE e.podcast_name = \"The AI Podcast\"\n",
    "    ORDER BY e.created_at\n",
    "''')\n",
    "\n",
    "ai_podcast_episodes = cursor.fetchall()\n",
    "\n",
    "print(f\"üìª The AI Podcast episodes:\")\n",
    "for ep_id, title, pub_date, guid, processed_at in ai_podcast_episodes:\n",
    "    print(f\"   ID {ep_id}: {title}\")\n",
    "    print(f\"      üìÖ Pub Date: {pub_date}\")\n",
    "    print(f\"      üîó GUID: {guid}\")\n",
    "    print(f\"      üß† Processed: {processed_at}\")\n",
    "    print()\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
